{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2019:][iapr2019] Special project\n",
    "\n",
    "**Group members:**\n",
    "    1- first name and last name,\n",
    "    2- first name and last name,\n",
    "    3- first name and last name\n",
    "\n",
    "**Due date:** 30.05.2019\n",
    "\n",
    "[iapr2019]: https://github.com/LTS5/iapr-2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Please find the description of this special project via [this link].\n",
    "\n",
    "[this link]: https://github.com/LTS5/iapr-2019/blob/master/project/special_project_description.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path_train = './project-data/images/train/'\n",
    "xml_path_train=\"./project-data/annotations/train/\"\n",
    "prepared_data_path=\"./project-data/images/train/prepared_data/\"\n",
    "img_list_dir = os.listdir(src_path_train)\n",
    "img_list = [names for names in img_list_dir if names.endswith(\".jpg\")]\n",
    "img_list_size = len(img_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_file(filename):\n",
    "    \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
    "    tree = ET.parse(filename)\n",
    "    objects = []\n",
    "    for obj in tree.findall('object'):\n",
    "        obj_struct = {}\n",
    "        obj_struct['name'] = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        obj_struct['bbox'] = [int(float(bbox.find('xmin').text)),\n",
    "                              int(float(bbox.find('ymin').text)),\n",
    "                              int(float(bbox.find('xmax').text))-int(float(bbox.find('xmin').text)),\n",
    "                              int(float(bbox.find('ymax').text))-int(float(bbox.find('ymin').text))]\n",
    "        objects.append(obj_struct)\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_xmls = [parse_file(os.path.join(xml_path_train, name[:-4]) + '.xml') for name in img_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Varroa', 'bbox': [1490, 815, 34, 34]}]\n",
      "58e7c5e3579e52085efb48f4_32.00px_0.jpg\n"
     ]
    }
   ],
   "source": [
    "print(parse_file(os.path.join(xml_path_train, img_list[0][:-4]) + '.xml'))\n",
    "print(img_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IOU(bbox1, bbox2):\n",
    "    #Calculate overlap between two bounding boxes [x, y, w, h] as the area of intersection over the area of unity\n",
    "    \n",
    "    #determine the (x, y)-coordinates of the intersection rectangle\n",
    "    x1, y1, w1, h1 = bbox1[0], bbox1[1], bbox1[2], bbox1[3]\n",
    "    x2, y2, w2, h2 = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n",
    "    \n",
    "    \n",
    "    xA = max(x1, x2)\n",
    "    yA = max(y1, y2)\n",
    "    xB = min(x1+w1, x2+w2)\n",
    "    yB = min(y1+h1, y2+h2)\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA ) * max(0, yB - yA )\n",
    "    \n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    boxAArea = (w1 ) * (h1 )\n",
    "    boxBArea = (w2 ) * (h2 )\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_label(img,img_name,annotations): \n",
    "    #Mirror padding\n",
    "    w,h,d = img.shape \n",
    "    w_i=int(w/32)+1\n",
    "    h_i=int(h/32)+1\n",
    "    new_w=w_i*32\n",
    "    new_h=h_i*32\n",
    "    count=0\n",
    "    new_img=img\n",
    "    l=0\n",
    "    for i in range(w_i): \n",
    "        for j in range(h_i): \n",
    "            count+=1 \n",
    "            name=img_name+\"_\"+str(count)\n",
    "            sub_img=new_img[0:64,0:64,:]\n",
    "            label=0\n",
    "            for box in annotations:\n",
    "                if (box[0]>=i*32  and box[0]< i*32+64 and box[1]>=j*32 and  box[1]< j*32+64) :\n",
    "                    if ( box[0]+box[2]<i*32+64 and box[1]+box[3]<j*32+64):\n",
    "                        l=l+1\n",
    "                        label=1\n",
    "                \"\"\"\"\n",
    "                iou=calculate_IOU(box,[i*32,j*32,64,64])\n",
    "                if (iou>0.5): \n",
    "                    label=1 \n",
    "                    j=j+1\n",
    "                    break\n",
    "                \"\"\"\n",
    "            name+=\"_\"+str(label)\n",
    "            im=Image.fromarray(sub_img)\n",
    "            b, g, r = im.split()\n",
    "            im= Image.merge(\"RGB\", (r, g, b))\n",
    "            im.save(prepared_data_path+name+\".jpg\")         \n",
    "            new_img=np.roll(new_img,-32,axis=1)\n",
    "        new_img=np.roll(new_img,-32,axis=0)\n",
    "    return l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_1=[]\n",
    "for j in range(len(annotations_xmls)):\n",
    "    for i in range(len(annotations_xmls[j])): \n",
    "        ground_truth_1.append(annotations_xmls[j][i]['bbox'][2:3])\n",
    "max_box_size=max(ground_truth_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def split_and_label_v2(prepared_data_path,img,img_name,annotations): \n",
    "    count=0\n",
    "    width,height=img.shape[0],img.shape[1]\n",
    "    bbox=[]\n",
    "    for box in annotations: \n",
    "        name=img_name+\"_\"+str(count)+\"_1\"\n",
    "\n",
    "        xmin,ymin,w,h=box[0],box[1],box[2],box[3] \n",
    "        max_size=int(max_box_size[0]/2)\n",
    "        new_img_=img[max(0,ymin-10):min(ymin+max_size+10,width),max(0,xmin-10):min(xmin+max_size+10,height),:]\n",
    "        im=Image.fromarray(new_img_)\n",
    "        bbox.append([10,10,box[2],box[3],1])\n",
    "        b, g, r = im.split()\n",
    "        im= Image.merge(\"RGB\", (r, g, b))\n",
    "        im.save(prepared_data_path+name+\".jpg\") \n",
    "        name=img_name+\"_\"+str(count)+\"_0\"\n",
    "        if (xmin-56)>0 and (ymin-56)>0:\n",
    "            x1 = random.randint(0, xmin-56)\n",
    "            y1 = random.randint(0, ymin-56)\n",
    "        else :\n",
    "            x1 = random.randint(0, min(xmin+56,height))\n",
    "            y1 = random.randint(0, min(ymin+56,width))\n",
    "        new_img=img[y1:min(y1+48,width),x1:min(height,x1+48),:]\n",
    "        bbox.append([0,0,0,0,0])\n",
    "        im=Image.fromarray(new_img)\n",
    "        b, g, r = im.split()\n",
    "        im= Image.merge(\"RGB\", (r, g, b))\n",
    "        im.save(prepared_data_path+name+\".jpg\") \n",
    "        count+=1\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox=[]\n",
    "for j in range(len(img_list)):\n",
    "    name=img_list[j]\n",
    "    img = cv2.imread(src_path_train+name)\n",
    "    ground_truth_1=[]\n",
    "    for i in range(len(annotations_xmls[j])): \n",
    "        ground_truth_1.append(annotations_xmls[j][i]['bbox'])\n",
    "    ground_truth_1\n",
    "    bbox.append(split_and_label_v2(prepared_data_path,img,name[:-4],ground_truth_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_path=\"./project-data/images/train/prepared_data/\"\n",
    "img_list_dir_prep = os.listdir(prepared_data_path)\n",
    "number_label_non_varroa=0\n",
    "number_label_varroa=0\n",
    "for prep_name in img_list_dir_prep: \n",
    "    if prep_name.endswith('_1.jpg'):\n",
    "        number_label_varroa+=1\n",
    "    if prep_name.endswith('_0.jpg'):\n",
    "        number_label_non_varroa+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images with label =1 (varroas) is  7483\n",
      "number of images with label =0 (non varroas) is  7483\n",
      "Total number of files =  14966\n"
     ]
    }
   ],
   "source": [
    "print(\"number of images with label =1 (varroas) is \",number_label_non_varroa)\n",
    "print(\"number of images with label =0 (non varroas) is \",number_label_varroa)\n",
    "print(\"Total number of files = \",number_label_varroa+number_label_non_varroa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_list_prep = [names for names in img_list_dir_prep if names.endswith(\".jpg\")]\n",
    "sizes = [Image.open(prepared_data_path+f, 'r').size for f in img_list_prep]\n",
    "max_width,max_height=max(sizes)\n",
    "for item in img_list_dir_prep:\n",
    "    if os.path.isfile(prepared_data_path+item):\n",
    "        im = Image.open(prepared_data_path+item)\n",
    "        f, e = os.path.splitext(prepared_data_path+item)\n",
    "        imResize = im.resize((max_width,max_height), Image.ANTIALIAS)\n",
    "        imResize.save(f + '.jpg', 'JPEG', quality=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    }
   ],
   "source": [
    "#\"Check if all images have the same size (48*48)\"\n",
    "img_list_prep = [names for names in img_list_dir_prep if names.endswith(\".jpg\")]\n",
    "sizes = [Image.open(prepared_data_path+f, 'r').size for f in img_list_prep]\n",
    "max_width,max_height=min(sizes)\n",
    "print(max_width,max_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs=number_label_varroa+number_label_non_varroa\n",
    "X_train=np.zeros((num_imgs,max_width,max_height,3))\n",
    "for i in range(len(img_list_prep)):\n",
    "    name=img_list_prep[i]\n",
    "    img = cv2.imread(prepared_data_path+name)\n",
    "    X_train[i,:,:,:]=img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14966, 48, 48, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "bbox_=np.zeros((X_train.shape[0],5))\n",
    "for image_box in bbox: \n",
    "    for box in image_box:\n",
    "        bbox_[count,0]=box[0]\n",
    "        bbox_[count,1]=box[1]\n",
    "        bbox_[count,2]=box[2]\n",
    "        bbox_[count,3]=box[3]\n",
    "        bbox_[count,4]=box[4]\n",
    "        count+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=bbox_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "num_epoch = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=1152, out_features=256, bias=True)\n",
      "  (box): Linear(in_features=256, out_features=4, bias=True)\n",
      "  (logit): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 8, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define network architecture\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output, n_c):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.box = torch.nn.Linear(n_hidden, n_output-1)   # output layer\n",
    "        self.logit = torch.nn.Linear(n_hidden, 1)\n",
    "        \n",
    "        self.conv1 = torch.nn.Sequential(         # \n",
    "            torch.nn.Conv2d(\n",
    "                in_channels = n_c,            # input height\n",
    "                out_channels = 8,             # n_filters\n",
    "                kernel_size = 2,              # filter size\n",
    "                stride = 2,                   # filter movement/step\n",
    "                padding = 0,                  \n",
    "            ),                              \n",
    "            torch.nn.ReLU(),                      # activation\n",
    "            #torch.nn.MaxPool2d(kernel_size = 2),    \n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(       \n",
    "            torch.nn.Conv2d(in_channels = 8, \n",
    "                            out_channels = 16, \n",
    "                            kernel_size = 2, \n",
    "                            stride = 2, \n",
    "                            padding = 0),      \n",
    "            torch.nn.ReLU(),                      # activation\n",
    "            #torch.nn.MaxPool2d(2),                \n",
    "        )\n",
    "        \n",
    "        self.conv3 = torch.nn.Sequential(       \n",
    "            torch.nn.Conv2d(in_channels = 16, \n",
    "                            out_channels = 8, \n",
    "                            kernel_size = 1, \n",
    "                            stride = 1, \n",
    "                            padding = 0),      \n",
    "            torch.nn.ReLU(),                      # activation\n",
    "            #torch.nn.MaxPool2d(2),                \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feat = self.conv1(x)\n",
    "        feat = self.conv2(feat)\n",
    "       \n",
    "        feat = self.conv3(feat)\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "        \n",
    "        x2 = F.relu(self.hidden(feat))      # activation function for hidden layer\n",
    "       \n",
    "        out_box = F.relu(self.box(x2))            # linear output\n",
    "        out_logit = torch.sigmoid(self.logit(x2))\n",
    "        \n",
    "        return out_box, out_logit\n",
    "      \n",
    "net = Net(n_feature = 1152, n_hidden = 256, n_output = 5, n_c = 3)     # define the network\n",
    "print(net)  # net architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_criterion =  torch.nn.MSELoss()\n",
    "classification_criterion =  torch.nn.BCELoss()# Hint: Consider that we only one class to predict\n",
    "gamma =0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  Total loss -> 20.10630   classif_loss -> 0.51692  regress_loss -> 20.30417\n",
      "epoch: 1  Total loss -> 14.80723   classif_loss -> 0.36578  regress_loss -> 14.95311\n",
      "epoch: 2  Total loss -> 13.66204   classif_loss -> 0.36956  regress_loss -> 13.79631\n",
      "epoch: 3  Total loss -> 12.80317   classif_loss -> 0.37386  regress_loss -> 12.92872\n",
      "epoch: 4  Total loss -> 12.38299   classif_loss -> 0.35231  regress_loss -> 12.50451\n",
      "epoch: 5  Total loss -> 11.78542   classif_loss -> 0.33599  regress_loss -> 11.90107\n",
      "epoch: 6  Total loss -> 11.29091   classif_loss -> 0.32308  regress_loss -> 11.40170\n",
      "epoch: 7  Total loss -> 10.72620   classif_loss -> 0.31120  regress_loss -> 10.83141\n",
      "epoch: 8  Total loss -> 9.98234   classif_loss -> 0.28956  regress_loss -> 10.08025\n",
      "epoch: 9  Total loss -> 9.51876   classif_loss -> 0.28284  regress_loss -> 9.61205\n"
     ]
    }
   ],
   "source": [
    "# Instanciate the network and define the optimizer\n",
    "num_channels=3\n",
    "net = Net(n_feature = 1152, n_hidden = 256, n_output = 5, n_c = 3)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "if(X_train.shape[1]!=num_channels): #dim1==channel\n",
    "    X_train = X_train.transpose((0,3,1,2))\n",
    "n_batch = X_train.shape[0]//batch_size\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch in range(n_batch):\n",
    "        batch_X = X_train[batch*batch_size:min((batch+1)*batch_size, X_train.shape[0])]\n",
    "        batch_y = Y_train[batch*batch_size:min((batch+1)*batch_size, X_train.shape[0])]\n",
    "        out_box, out_logit = net(torch.tensor(batch_X, dtype=torch.float32))\n",
    "        \n",
    "        mask_arr = np.argwhere(batch_y[:,-1]==1).reshape((-1,))\n",
    "        regression_loss = regression_criterion(out_box[mask_arr], torch.tensor(batch_y[mask_arr,:-1], dtype=torch.float32))\n",
    "        classification_loss = classification_criterion(out_logit, torch.tensor(batch_y[:,-1:], dtype=torch.float32))\n",
    "        \n",
    "        # Compose the 2 loss functions using the weight gamma (1 line)\n",
    "        loss = gamma*(regression_loss)+(1-gamma)*classification_loss \n",
    "        \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "    \n",
    "    print('epoch: {}  Total loss -> {:.5f}   classif_loss -> {:.5f}  regress_loss -> {:.5f}'\n",
    "          .format(epoch, loss.item(), classification_loss.item(), regression_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(pred_class, pred_bboxes, test_bboxes):\n",
    "    # Calculate the mean IOU (overlap) between the predicted and expected bounding boxes on the test dataset. \n",
    "    summed_IOU = 0.\n",
    "    l =0\n",
    "    for pred_bbox, test_bbox in zip(pred_bboxes.reshape(-1, 4), test_bboxes.reshape(-1, 5)):\n",
    "        if(test_bbox[4]==1): # the ones that have black boxes\n",
    "            summed_IOU += calculate_IOU(pred_bbox, test_bbox)\n",
    "            l+=1\n",
    "    mean_IOU = summed_IOU / l\n",
    "    print(\"mean IOU: \", mean_IOU)\n",
    "\n",
    "    # classification accuracy\n",
    "    print(\"classification acc: \", np.mean(test_bboxes[:,4:]==pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10., 10., 34., 34.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [10., 10., 33., 33.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [10., 10., 21., 21.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean IOU:  0.5703976086993328\n",
      "classification acc:  0.9045837231057062\n"
     ]
    }
   ],
   "source": [
    "if(X_train.shape[1]!=num_channels):\n",
    "    X_train = train_X.transpose((0,3,1,2))\n",
    "# Predict bounding boxes on the train images.\n",
    "with torch.no_grad():\n",
    "    pred_y_train_box, pred_y_train_logit = net.forward(torch.tensor(X_train, dtype=torch.float32))\n",
    "    pred_y_train_box, pred_y_train_logit = pred_y_train_box.numpy(), pred_y_train_logit.numpy()\n",
    "    pred_y_train_label = pred_y_train_logit>0.5\n",
    "    pred_bboxes_train = pred_y_train_box\n",
    "    pred_bboxes_train = pred_bboxes_train.reshape(len(pred_bboxes_train), 1, -1)\n",
    "    pred_bboxes_train.shape\n",
    "\n",
    "calculate_accuracy(pred_y_train_label, pred_bboxes_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "## prepare the testing dataset \n",
    "src_path_test = './project-data/images/test/'\n",
    "xml_path_test=\"./project-data/annotations/test/\"\n",
    "prepared_data_path_test=\"./project-data/images/test/prepared_data/\"\n",
    "img_list_dir_test = os.listdir(src_path_test)\n",
    "img_list_test = [names for names in img_list_dir_test  if names.endswith(\".jpg\")]\n",
    "img_list_size_test = len(img_list_test)\n",
    "print(img_list_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_xmls_test = [parse_file(os.path.join(xml_path_test, name[:-4]) + '.xml') for name in img_list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./project-data/images/test/'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_path_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_test=[]\n",
    "for j in range(len(img_list_test)):\n",
    "    name=img_list_test[j]\n",
    "    img = cv2.imread(src_path_test+name)\n",
    "    ground_truth_1=[]\n",
    "    for i in range(len(annotations_xmls_test[j])): \n",
    "        ground_truth_1.append(annotations_xmls_test[j][i]['bbox'])\n",
    "    ground_truth_1\n",
    "    bbox_test.append(split_and_label_v2(prepared_data_path_test,img,name[:-4],ground_truth_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list_prep_test = [names for names in img_list_dir_prep if names.endswith(\".jpg\")]\n",
    "sizes = [Image.open(prepared_data_path_test+f, 'r').size for f in img_list_prep_test]\n",
    "max_width,max_height=max(sizes)\n",
    "for item in img_list_dir_prep:\n",
    "    if os.path.isfile(prepared_data_path+item):\n",
    "        im = Image.open(prepared_data_path+item)\n",
    "        f, e = os.path.splitext(prepared_data_path+item)\n",
    "        imResize = im.resize((max_width,max_height), Image.ANTIALIAS)\n",
    "        imResize.save(f + '.jpg', 'JPEG', quality=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Finding varroas by segmentation\n",
    "Add your implementation for ''**detect_by_segmentation**'' function. Please make sure the input and output follows the mentioned format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_by_segmentation(img):\n",
    "    '''\n",
    "    Input: One single image\n",
    "    Output: A numpy array containing coordonates of all detected varroas, with the following format: \n",
    "            [[x_1, y_1, w_1, h_2], [x_2, y_2, w_1, h_2], ..., [x_n, y_n, w_n, h_n]] \n",
    "            where ''n'' is the number of detected varroas.\n",
    "    '''\n",
    "\n",
    "    #Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your implementation. Report the Precision, Recall and F1-score, by using all 50 images of the test-set, and considering 0.3 as the IoU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement your first detector\n",
    "\n",
    "Write your function(s) for the second part. Feel free to change the name of the function and add your additional functions, but please make sure their input and output follows the mentioned format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_by_method_1(img):\n",
    "    '''\n",
    "    Input: One single image\n",
    "    Output: A numpy array containing coordonates of all detected varroas, with the following format: \n",
    "            [[x_1, y_1, w_1, h_2], [x_2, y_2, w_1, h_2], ..., [x_n, y_n, w_n, h_n]] \n",
    "            where ''n'' is the number of detected varroas.\n",
    "    '''\n",
    "\n",
    "    #Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your implementation. Report the Precision, Recall and F1-score, by using all 50 images of the test-set, and considering 0.3 as the IoU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using MLP and CNNs\n",
    "\n",
    "Add your implementation for the thrid part. Feel free to add your desirable functions, but please make sure you have proper functions for the final detection, where their input and output follows the same format as the previous parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You can generate a json submission file by using the function ''**generate_pred_json**''. This prediction file can be uploaded online for evaluation (Please refer to section 3 of the project description for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def generate_pred_json(data, tag='baseline'):\n",
    "    '''\n",
    "    Input\n",
    "    - data: Is a dictionary d, such that:\n",
    "          d = { \n",
    "              \"ID_1\": [], \n",
    "              \"ID_2\": [[x_21, y_21, w_21, h_21], [x_22, y_22, w_22, h_22]], \n",
    "              ... \n",
    "              \"ID_i\": [[x_i1, y_i1, w_i1, h_i1], ..., [x_iJ, y_iJ, w_iJ, h_iJ]],\n",
    "              ... \n",
    "              \"ID_N\": [[x_N1, y_N1, w_N1, h_N1]],\n",
    "          }\n",
    "          where ID is the string id of the image (e.i. 5a05e86fa07d56baef59b1cb_32.00px_1) and the value the Kx4 \n",
    "          array of intergers for the K predicted bounding boxes (e.g. [[170, 120, 15, 15]])\n",
    "    - tag: (optional) string that will be added to the name of the json file.\n",
    "    Output\n",
    "      Create a json file, \"prediction_[tag].json\", conatining the prediction to EvalAI format.\n",
    "    '''\n",
    "    unvalid_key = []\n",
    "    _data = data.copy()\n",
    "    for key, value in _data.items():\n",
    "        try:\n",
    "            # Try to convert to numpy array and cast as closest int\n",
    "            print(key)\n",
    "            v = np.around(np.array(value)).astype(int)\n",
    "            # Check is it is a 2d array with 4 columns (x,y,w,h)\n",
    "            if v.ndim != 2 or v.shape[1] != 4:\n",
    "                unvalid_key.append(key)\n",
    "            # Id must be a string\n",
    "            if not isinstance(key, str):\n",
    "                unvalid_key.append(key)\n",
    "            _data[key] = v.tolist()\n",
    "        # Deal with not consistant array size and empty predictions\n",
    "        except (ValueError, TypeError):\n",
    "            unvalid_key.append(key)\n",
    "    # Remove unvalid key from dictionnary\n",
    "    for key in unvalid_key: del _data[key]\n",
    "    \n",
    "    with open('prediction_{}.json'.format(tag), 'w') as outfile:\n",
    "        json.dump(_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
