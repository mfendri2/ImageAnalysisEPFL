{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2019:][iapr2019] Special project\n",
    "\n",
    "**Group members:**\n",
    "    1- first name and last name,\n",
    "    2- first name and last name,\n",
    "    3- first name and last name\n",
    "\n",
    "**Due date:** 30.05.2019\n",
    "\n",
    "[iapr2019]: https://github.com/LTS5/iapr-2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Please find the description of this special project via [this link].\n",
    "\n",
    "[this link]: https://github.com/LTS5/iapr-2019/blob/master/project/special_project_description.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hedi Fendri\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import warnings\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,auc,roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import  average_precision_score\n",
    "import matplotlib.patches as patches\n",
    "from sklearn import preprocessing\n",
    "from scipy.fftpack import fft, ifft\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import math\n",
    "import cv2 \n",
    "import xml.etree.ElementTree as ET\n",
    "from skimage import feature\n",
    "from skimage import transform\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining paths to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image size 799 , test image size 50, validation image size 150\n"
     ]
    }
   ],
   "source": [
    "src_path_train = './project-data/images/train/'\n",
    "xml_path_train=\"./project-data/annotations/train/\"\n",
    "prepared_data_path=\"./project-data/images/train/prepared_data/\"\n",
    "img_list_dir = os.listdir(src_path_train)\n",
    "img_list = [names for names in img_list_dir if names.endswith(\".jpg\")]\n",
    "img_list_size = len(img_list)\n",
    "\n",
    "src_path_test = './project-data/images/test/'\n",
    "xml_path_test=\"./project-data/annotations/test/\"\n",
    "prepared_data_path_test=\"./project-data/images/test/prepared_data/\"\n",
    "img_list_dir_test = os.listdir(src_path_test)\n",
    "img_list_test = [names for names in img_list_dir_test  if names.endswith(\".jpg\")]\n",
    "img_list_size_test = len(img_list_test)\n",
    "\n",
    "src_path_validation = './project-data/images/validation/'\n",
    "xml_path_validation=\"./project-data/annotations/validation/\"\n",
    "prepared_data_path_validation=\"./project-data/images/validation/prepared_data/\"\n",
    "img_list_dir_validation = os.listdir(src_path_validation)\n",
    "img_list_validation = [names for names in img_list_dir_validation  if names.endswith(\".jpg\")]\n",
    "img_list_size_validation = len(img_list_validation)\n",
    "print(\"train image size {} , test image size {}, validation image size {}\".format(img_list_size,img_list_size_test,img_list_size_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Xml files to save ground truths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(filename):\n",
    "    \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
    "    tree = ET.parse(filename)\n",
    "    objects = []\n",
    "    for obj in tree.findall('object'):\n",
    "        obj_struct = {}\n",
    "        obj_struct['name'] = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        obj_struct['bbox'] = [int(float(bbox.find('xmin').text)),\n",
    "                              int(float(bbox.find('ymin').text)),\n",
    "                              int(float(bbox.find('xmax').text))-int(float(bbox.find('xmin').text)),\n",
    "                              int(float(bbox.find('ymax').text))-int(float(bbox.find('ymin').text))]\n",
    "        objects.append(obj_struct)\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_xmls = [parse_file(os.path.join(xml_path_train, name[:-4]) + '.xml') for name in img_list]\n",
    "annotations_xmls_test=[parse_file(os.path.join(xml_path_test, name[:-4]) + '.xml') for name in img_list_test]\n",
    "annotations_xmls_validation=[parse_file(os.path.join(xml_path_validation, name[:-4]) + '.xml') for name in img_list_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IOU(bbox1, bbox2):\n",
    "    #Calculate overlap between two bounding boxes [x, y, w, h] as the area of intersection over the area of unity\n",
    "    \n",
    "    #determine the (x, y)-coordinates of the intersection rectangle\n",
    "    x1, y1, w1, h1 = bbox1[0], bbox1[1], bbox1[2], bbox1[3]\n",
    "    x2, y2, w2, h2 = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n",
    "    \n",
    "    \n",
    "    xA = max(x1, x2)\n",
    "    yA = max(y1, y2)\n",
    "    xB = min(x1+w1, x2+w2)\n",
    "    yB = min(y1+h1, y2+h2)\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA ) * max(0, yB - yA )\n",
    "    \n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    boxAArea = (w1 ) * (h1 )\n",
    "    boxBArea = (w2 ) * (h2 )\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou\n",
    "def sliding_window(image, stepSize, windowSize):\n",
    "    \"\"\"\n",
    "    This function perform sliding_window on the image whith a corresponding stepsize \n",
    "    ---------------------------------------------------------\n",
    "    Inputs: \n",
    "        image \n",
    "        stepSize : the desired step between two images : (<48 = overlap) (>48 : no overlap)\n",
    "        WindowSize: for our case should be : (48,48)\n",
    "    Outputs :\n",
    "        None     \n",
    "    \"\"\"\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0], stepSize):\n",
    "        for x in range(0, image.shape[1], stepSize):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])\n",
    "            \n",
    "def non_max_suppression(boxes, probs=None, overlapThresh=0.3):\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # if the bounding boxes are integers, convert them to floats -- this\n",
    "    # is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    # compute the area of the bounding boxes and grab the indexes to sort\n",
    "    # (in the case that no probabilities are provided, simply sort on the\n",
    "    # bottom-left y-coordinate)\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = y2\n",
    "\n",
    "    # if probabilities are provided, sort on them instead\n",
    "    if probs is not None:\n",
    "        idxs = probs\n",
    "\n",
    "    # sort the indexes\n",
    "    idxs = np.argsort(idxs)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the index value\n",
    "        # to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the largest (x, y) coordinates for the start of the bounding\n",
    "        # box and the smallest (x, y) coordinates for the end of the bounding\n",
    "        # box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        # delete all indexes from the index list that have overlap greater\n",
    "        # than the provided overlap threshold\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "\n",
    "    # return only the bounding boxes that were picked\n",
    "    return boxes[pick].astype(\"int\")\n",
    "def plot_predictedVsGround(im,ground_truth,predicted_varroa):\n",
    "    \"\"\"\"\n",
    "    This function plot the ground_truth Vs predicted_Varroas as rectangles \n",
    "    \n",
    "    Inputs\n",
    "    -------------------------------------------------------------------------------------------------------\n",
    "        im: numpy array of the image where to plot the rectangles : this can be the input original binary image \n",
    "        ground_truth : list of ground truth varroas rectangles \n",
    "        predicted_varroas : list of predicted_varroas rectangles\n",
    "    -------------------------------------------------------------------------------------------------------\n",
    "    return : \n",
    "       NONE\n",
    "    \"\"\"     \n",
    "\n",
    "    fig,ax = plt.subplots(figsize=((7,7)))\n",
    "    ax.set_title(\"Ground truth varroas vs predicted varroas\")\n",
    "    ax.imshow(im)\n",
    "    green_rect=[]\n",
    "    red_rect=[]\n",
    "    for i in range(len(predicted_varroa)):\n",
    "        green_rect=patches.Rectangle((predicted_varroa[i][0],predicted_varroa[i][1]),predicted_varroa[i][2],predicted_varroa[i][3],facecolor='none',linewidth=1,edgecolor='g',label='Predicted varroas') \n",
    "\n",
    "        ax.add_patch(green_rect)\n",
    "    for box in ground_truth: \n",
    "        red_rect = patches.Rectangle((box[0], box[1]), box[2], box[3],\n",
    "                                 linewidth=1,edgecolor='r',facecolor='none',label='Ground trurh')\n",
    "        ax.add_patch(red_rect)\n",
    "        ax.legend(handles=[red_rect,green_rect])\n",
    "        \n",
    "def compute_f1(boxes_found,ground_truth,screen=0):\n",
    "    \"\"\"\n",
    "    Calculate the F_measure on the predicted box and the ground truth on a threshold from 0.1 to 0.5 \n",
    "    --------------\n",
    "    Inputs : \n",
    "        boxes_found : predicted box (e.g: [x,y,w,h]) \n",
    "        ground_truh: \n",
    "        screen: O: print  results 1: Withou printing results \n",
    "    \"\"\"\n",
    "    f_measure=[]\n",
    "\n",
    "    for thresh in [0.1,0.2,0.3,0.4,0.5]:\n",
    "        positive = 0\n",
    "        f_neg=0\n",
    "        for bf in boxes_found:\n",
    "            for atb in ground_truth:\n",
    "                iou = calculate_IOU(bf,atb)\n",
    "                if iou > thresh:\n",
    "                    positive+=1\n",
    "        if positive>len(boxes_found):\n",
    "            positive=len(boxes_found)\n",
    "        for bf in ground_truth :\n",
    "            iou=np.zeros((len(boxes_found),2))\n",
    "            j=0\n",
    "            for atb in boxes_found :\n",
    "                iou[j]=calculate_IOU(bf,atb)\n",
    "                j=j+1\n",
    "            if (iou.sum()==0):\n",
    "                f_neg+=1\n",
    "        \n",
    "        precision = 0.\n",
    "        recall = 0.\n",
    "        if len(boxes_found)>0:\n",
    "            precision = positive/len(boxes_found)\n",
    "        if (positive+f_neg)>0:\n",
    "            recall = positive/(positive+f_neg)\n",
    "\n",
    "        f1 = 0.\n",
    "        if (precision+recall) > 0:\n",
    "            f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "        if len(boxes_found) + len(ground_truth) == 0:\n",
    "            f1 = 1\n",
    "        if screen ==1: \n",
    "            print(\"TP with threshold {} =\".format(thresh),positive)\n",
    "            print(\"FP with threshold {} =\".format(thresh),len(boxes_found)-positive)\n",
    "            print(\"FN with threshold {} =\".format(thresh),f_neg)\n",
    "            print(\"F_measure for image with threshold={} =\".format(thresh),f1)\n",
    "        f_measure.append(f1)\n",
    "    if screen==1:\n",
    "        print(\"Mean F Measure =\",np.mean(f_measure))\n",
    "    return np.mean(f_measure)\n",
    "def f1_measure(TP,FP,FN):\n",
    "        precision = 0.\n",
    "        recall = 0.\n",
    "        if TP+FP>0:\n",
    "            precision = positive/(TP+FP)\n",
    "        if (TP+FN)>0:\n",
    "            recall = positive/(TP+FN)\n",
    "\n",
    "        f1 = 0.\n",
    "        if (precision+recall) > 0:\n",
    "            f1 = 2*precision*recall/(precision+recall)\n",
    "        return f1\n",
    "def compute_confusion_matrix(boxes_found,ground_truth,iou_threshold):\n",
    "    \"\"\"\n",
    "    Calculate the F_measure on the predicted box and the ground truth on a threshold from 0.1 to 0.5 \n",
    "    --------------\n",
    "    Inputs : \n",
    "        boxes_found : predicted box (e.g: [x,y,w,h]) \n",
    "        ground_truh: \n",
    "        screen: O: print  results 1: Withou printing results \n",
    "    \"\"\"\n",
    "\n",
    "    positive = 0\n",
    "    f_neg=0\n",
    "    for bf in boxes_found:\n",
    "        for atb in ground_truth:\n",
    "            iou = calculate_IOU(bf,atb)\n",
    "            if iou > iou_threshold:\n",
    "                positive+=1\n",
    "    if positive>len(boxes_found):\n",
    "        positive=len(boxes_found)\n",
    "    \n",
    "    f_neg=len(ground_truth)-positive\n",
    "    false_positive=len(boxes_found)-positive\n",
    "    return (positive,false_positive,f_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Finding varroas by segmentation\n",
    "Add your implementation for ''**detect_by_segmentation**'' function. Please make sure the input and output follows the mentioned format. Report the Precision, Recall and F1-score, by using all 50 images of the test-set, and considering 0.3 as the IoU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ae7d9e29b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ref_im=cv2.imread(\"var_temp.jpg\")\n",
    "varroa_gray = cv2.cvtColor(ref_im, cv2.COLOR_BGR2GRAY)\n",
    "varroa_gray=varroa_gray\n",
    "plt.imshow(varroa_gray,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_by_segmentation(img,thres_corr):\n",
    "    '''\n",
    "    Input: One single image\n",
    "    Output: A numpy array containing coordonates of all detected varroas, with the following format: \n",
    "            [[x_1, y_1, w_1, h_2], [x_2, y_2, w_1, h_2], ..., [x_n, y_n, w_n, h_n]] \n",
    "            where ''n'' is the number of detected varroas.\n",
    "    '''\n",
    "    \n",
    "    #grayscale\n",
    "    #img_gray = rgb2gray(img)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #convolution with varroa\n",
    "    #correlation = match_template(img_gray, varroa_gray)\n",
    "    correlation = cv2.matchTemplate(img_gray, varroa_gray,cv2.TM_CCORR_NORMED)\n",
    "    \n",
    "    result = correlation > thres_corr\n",
    "    \n",
    "    #find objects\n",
    "    labels = measure.label(result)\n",
    "    props = measure.regionprops(labels)\n",
    "    \n",
    "    #get box for all objects\n",
    "    boxes = []\n",
    "    for prop in props:\n",
    "        boxes.append([prop.bbox[1],prop.bbox[0],40,40])\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes(image,boxes_1,boxes_2):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "    for b in boxes_1:\n",
    "        rect = patches.Rectangle((b[0], b[1]), b[2], b[3],linewidth=2,edgecolor='r',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    for b in boxes_2:\n",
    "        rect = patches.Rectangle((b[0], b[1]), b[2], b[3],linewidth=2,edgecolor='g',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||||||||||||||||||||||||||||||||||||||||||||||||finished !\n",
      "0.2675324675324675\n"
     ]
    }
   ],
   "source": [
    "import skimage.io\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "tot_true_pos = 0.\n",
    "tot_found = 0.\n",
    "tot_true = 0.\n",
    "\n",
    "for x in range(img_list_size_test):\n",
    "    #image = imread(src_path_test+img_list_test[x])\n",
    "    image = cv2.imread(src_path_test+img_list_test[x])\n",
    "\n",
    "    annotations_test_boxes = []\n",
    "    L = len(annotations_xmls_test[x])\n",
    "    for i in range(L):\n",
    "        annotations_test_boxes.append(annotations_xmls_test[x][i]['bbox'])\n",
    "\n",
    "    boxes_found = detect_by_segmentation(image,0.96)\n",
    "\n",
    "    #plot_boxes(image,annotations_test_boxes,boxes_found)\n",
    "\n",
    "    for bf in boxes_found:\n",
    "        for atb in annotations_test_boxes:\n",
    "            iou = calculate_IOU(bf,atb)\n",
    "            if iou > 0.3:\n",
    "                tot_true_pos+=1\n",
    "    \n",
    "    tot_found += len(boxes_found)\n",
    "    tot_true += len(annotations_test_boxes)\n",
    "       \n",
    "    print('|',end='')\n",
    "\n",
    "print('finished !')\n",
    "precision = tot_true_pos/tot_found\n",
    "recall = tot_true_pos/tot_true\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL MEAN F_MEASURE for each image seperately = 0.4193420007089583\n"
     ]
    }
   ],
   "source": [
    "number=-1\n",
    "F_measure=[]\n",
    "for x in range(50):\n",
    "    best_corr=0\n",
    "    ground_truth_1=[]\n",
    "    for i in range(len(annotations_xmls_test[x])): \n",
    "        ground_truth_1.append(annotations_xmls_test[x][i]['bbox'])\n",
    "    number+=1\n",
    "    #image = imread(src_path_test+img_list_test[x])\n",
    "    image = cv2.imread(src_path_test+img_list_test[x])\n",
    "    f1_old=0\n",
    "    for cor in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "        boxes_found = detect_by_segmentation(image,thres_corr=cor)\n",
    "        f1=compute_f1(boxes_found,ground_truth_1,0)\n",
    "        if f1>f1_old:\n",
    "            best_corr=cor\n",
    "            f1_old=f1\n",
    "    boxes_found = detect_by_segmentation(image,thres_corr=best_corr)\n",
    "    f1_final=compute_f1(boxes_found,ground_truth_1,0)\n",
    "    #print(\"f1 measure for image {} is \".format(x),f1_final)\n",
    "    F_measure.append(f1_final)\n",
    "    #plot_predictedVsGround(image,ground_truth_1,boxes_found)\n",
    "print(\"TOTAL MEAN F_MEASURE for each image seperately =\",np.mean(F_measure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement your first detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your function(s) for the second part. Feel free to change the name of the function and add your additional functions, but please make sure their input and output follows the mentioned format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation : Build the dataset (training, validation and testing )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to build the dataset : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def split_and_label_v2(prepared_data_path,max_box_size,img,img_name,annotations):\n",
    "    \"\"\"\n",
    "    This function allow us to take the region on interests from the training , validation and testing data : \n",
    "    Take a window of 48,48 of the varroas and one non varroa image selected from the remaining part of the image \n",
    "    randomly. \n",
    "    ------------------------------------\n",
    "    Inputs :\n",
    "        prepared_data_path : Path to save the result images \n",
    "        max_box_size: the maximum box sizes in the dataset \n",
    "        img : the image to be splitted \n",
    "        img_name : image name \n",
    "        annotations : Ground truth of the image (Varroas boxes)\n",
    "    Outputs : \n",
    "        None\n",
    "    \"\"\"\n",
    "    count=0\n",
    "    width,height=img.shape[0],img.shape[1]\n",
    "    \n",
    "    for box in annotations: \n",
    "        name=img_name+\"_\"+str(count)+\"_1\"\n",
    "\n",
    "        xmin,ymin,w,h=box[0],box[1],box[2],box[3] \n",
    "        max_size=int(max_box_size[0]/2)\n",
    "        new_img_=img[max(0,ymin-10):min(ymin+max_size+10,width),max(0,xmin-10):min(xmin+max_size+10,height),:]\n",
    "        im=Image.fromarray(new_img_)\n",
    "        b, g, r = im.split()\n",
    "        im= Image.merge(\"RGB\", (r, g, b))\n",
    "        im.save(prepared_data_path+name+\".jpg\") \n",
    "        #for i in range(2):\n",
    "        name=img_name+\"_\"+str(count)+\"_0\"\n",
    "        x1 = random.randint(0,height-48)\n",
    "        y1 = random.randint(0,width-48)\n",
    "        new_img=img[y1:min(y1+48,width),x1:min(height,x1+48),:]\n",
    "        im=Image.fromarray(new_img)\n",
    "        b, g, r = im.split()\n",
    "        im= Image.merge(\"RGB\", (r, g, b))\n",
    "        im.save(prepared_data_path+name+\".jpg\") \n",
    "        count+=1\n",
    "def make_dataset(img_list,src_path,max_box_size,prepared_data_path,annotations_xmls):\n",
    "    \"\"\"\n",
    "    This function build the complete dataset for training , testing and validation \n",
    "    ---------------------------------------\n",
    "    Inputs : \n",
    "        img_list : the list of images in the training or validation or testing \n",
    "        max_box_size : the maximum box size in the images \n",
    "        prepared_data_path : Path where to save the result images \n",
    "        annotations_xmls : The result of parsing the xml file \n",
    "    Outputs:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for j in range(len(img_list)):\n",
    "        name=img_list[j]\n",
    "        img = cv2.imread(src_path+name)\n",
    "        ground_truth_1=[]\n",
    "        for i in range(len(annotations_xmls[j])): \n",
    "            ground_truth_1.append(annotations_xmls[j][i]['bbox'])\n",
    "        ground_truth_1\n",
    "        split_and_label_v2(prepared_data_path,max_box_size,img,name[:-4],ground_truth_1)\n",
    "\n",
    "def resize(img_list_dir_prep,prepared_data_path):\n",
    "    \"\"\"\n",
    "    This function allows to have the same dimension of the images and fixed to (48,48)\n",
    "    ------------------------\n",
    "    Inputs :\n",
    "        img_list_dir_prep : image names of the images to be resized \n",
    "        prepared_data_path: Path where to save the resized images \n",
    "    Outputs : \n",
    "        None\n",
    "    \"\"\"\n",
    "    img_list_prep = [names for names in img_list_dir_prep if names.endswith(\".jpg\")]\n",
    "    sizes = [Image.open(prepared_data_path+f, 'r').size for f in img_list_prep]\n",
    "    max_width,max_height=max(sizes)\n",
    "    for item in img_list_dir_prep:\n",
    "        if os.path.isfile(prepared_data_path+item):\n",
    "            im = Image.open(prepared_data_path+item)\n",
    "            \n",
    "            f, e = os.path.splitext(prepared_data_path+item)\n",
    "            imResize = im.resize((48,48), Image.ANTIALIAS)\n",
    "            imResize.save(f + '.jpg', 'JPEG', quality=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method(y_true,y_pred): \n",
    "    \"\"\" Evaluating the model using f-measure and accuracy\n",
    "     Parameters\n",
    "    ----------\n",
    "        y_true  : True labels\n",
    "\n",
    "        y_pred  : Predicted labels\n",
    "                    \n",
    "    Returns : None\n",
    "    \n",
    "    \"\"\"\n",
    "    f1=metrics.f1_score(y_true, y_pred)\n",
    "    acc=metrics.accuracy_score(y_true, y_pred)\n",
    "    recall=metrics.recall_score(y_true, y_pred)\n",
    "    precision=metrics.precision_score(y_true, y_pred)\n",
    "    print(\"Recall= \",recall)\n",
    "    print(\"precision= \",precision)\n",
    "    print(\"F-measure= \",f1)\n",
    "    print(\"Accuracy= \",acc)\n",
    "    \n",
    "def plot_roc(y_true,y_pred):\n",
    "    \"\"\" Plotting Roc curve \n",
    "     Parameters\n",
    "    ----------\n",
    "        y_true  : True labels\n",
    "\n",
    "        y_pred  : Predicted labels\n",
    "                    \n",
    "    Returns : None\n",
    "    \n",
    "    \"\"\"\n",
    "    auc=roc_auc_score(y_true, y_pred)\n",
    "    print('auc=',auc)\n",
    "    fpr,tpr,_=roc_curve(y_true, y_pred)\n",
    "    plt.plot(fpr,tpr)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    t=plt.title('Receiver operating characteristic')\n",
    "    \n",
    "def plot_prec_recall_curve(y_true, y_pred):\n",
    "    precision, recall, thresholds=precision_recall_curve(y_true, y_pred)\n",
    "    plt.plot(recall,precision)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    t=plt.title('Precision recall curve')\n",
    "    print(\"AUPR= \",average_precision_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the methods let's build our dataset : <br> \n",
    "We already build the dataset : do not run the code again. \n",
    "You can ran the code after creating new folding in the corresponding (train , validation , test) folder (eg. folder named\n",
    "prepared_data and pass the path to the function) Make dataset function will call the function split and label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define first in the cell bellow : the maximum varroas sizes for training , validation and testing : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train max box size [56] , test max box size [52], validation max box size [52]\n"
     ]
    }
   ],
   "source": [
    "ground_truth_1=[]\n",
    "for j in range(len(annotations_xmls)):\n",
    "    for i in range(len(annotations_xmls[j])): \n",
    "        ground_truth_1.append(annotations_xmls[j][i]['bbox'][2:3])\n",
    "max_box_size=max(ground_truth_1)\n",
    "ground_truth_1=[]\n",
    "\n",
    "for j in range(len(annotations_xmls_test)):\n",
    "    for i in range(len(annotations_xmls_test[j])): \n",
    "        ground_truth_1.append(annotations_xmls_test[j][i]['bbox'][2:3])\n",
    "max_box_size_test=max(ground_truth_1)\n",
    "\n",
    "for j in range(len(annotations_xmls_validation)):\n",
    "    for i in range(len(annotations_xmls_validation[j])): \n",
    "        ground_truth_1.append(annotations_xmls_validation[j][i]['bbox'][2:3])\n",
    "max_box_size_validation=max(ground_truth_1)\n",
    "print(\"train max box size {} , test max box size {}, validation max box size {}\".format(max_box_size,max_box_size_test,max_box_size_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmake_dataset(img_list,src_path_train,max_box_size,prepared_data_path,annotations_xmls)\\nmake_dataset(img_list_test,src_path_test,max_box_size_test,prepared_data_path_test,annotations_xmls_test)\\nmake_dataset(img_list_validation,src_path_validation,max_box_size_validation,prepared_data_path_validation,annotations_xmls_validation)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "make_dataset(img_list,src_path_train,max_box_size,prepared_data_path,annotations_xmls)\n",
    "make_dataset(img_list_test,src_path_test,max_box_size_test,prepared_data_path_test,annotations_xmls_test)\n",
    "make_dataset(img_list_validation,src_path_validation,max_box_size_validation,prepared_data_path_validation,annotations_xmls_validation)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to mention here that the dataset is build balanced in the very begining , We  train on the training and test on the testing dataset. Afterwords the detected False positive is added to the training dataset to end up with 36% of varroas and 64 % of non varroas. We repeated the process of training and adding the false positive 3 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will resize our images to the same size (48,48) : </br> \n",
    "We already resized the dataset , Please run the following code only one time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR TRAINING\n",
      "number of images with label =1 (varroas) is  7493\n",
      "number of images with label =0 (non varroas) is  12213\n",
      "Total number of files =  19706\n",
      "FOR TESTING\n",
      "number of images with label =1 (varroas) is  582\n",
      "number of images with label =0 (non varroas) is  582\n",
      "Total number of files =  1164\n",
      "FOR VALIDATION\n",
      "number of images with label =1 (varroas) is  1539\n",
      "number of images with label =0 (non varroas) is  1539\n",
      "Total number of files =  3078\n"
     ]
    }
   ],
   "source": [
    "img_list_dir_prep = os.listdir(prepared_data_path)\n",
    "number_label_non_varroa_train=0\n",
    "number_label_varroa_train=0\n",
    "for prep_name in img_list_dir_prep: \n",
    "    if prep_name.endswith('_1.jpg'):\n",
    "        number_label_varroa_train+=1\n",
    "    if prep_name.endswith('_0.jpg'):\n",
    "        number_label_non_varroa_train+=1\n",
    "print(\"FOR TRAINING\")\n",
    "print(\"number of images with label =1 (varroas) is \",number_label_varroa_train)\n",
    "print(\"number of images with label =0 (non varroas) is \",number_label_non_varroa_train)\n",
    "print(\"Total number of files = \",number_label_varroa_train+number_label_non_varroa_train)    \n",
    "\n",
    "img_list_dir_prep_test = os.listdir(prepared_data_path_test)\n",
    "number_label_non_varroa_test=0\n",
    "number_label_varroa_test=0\n",
    "for prep_name in img_list_dir_prep_test: \n",
    "    if prep_name.endswith('_1.jpg'):\n",
    "        number_label_varroa_test+=1\n",
    "    if prep_name.endswith('_0.jpg'):\n",
    "        number_label_non_varroa_test+=1\n",
    "print(\"FOR TESTING\")\n",
    "print(\"number of images with label =1 (varroas) is \",number_label_varroa_test)\n",
    "print(\"number of images with label =0 (non varroas) is \",number_label_non_varroa_test)\n",
    "print(\"Total number of files = \",number_label_varroa_test+number_label_non_varroa_test)\n",
    "\n",
    "img_list_dir_prep_validation = os.listdir(prepared_data_path_validation)\n",
    "number_label_non_varroa_val=0\n",
    "number_label_varroa_val=0\n",
    "for prep_name in img_list_dir_prep_validation: \n",
    "    if prep_name.endswith('_1.jpg'):\n",
    "        number_label_varroa_val+=1\n",
    "    if prep_name.endswith('_0.jpg'):\n",
    "        number_label_non_varroa_val+=1\n",
    "print(\"FOR VALIDATION\")\n",
    "print(\"number of images with label =1 (varroas) is \",number_label_varroa_val)\n",
    "print(\"number of images with label =0 (non varroas) is \",number_label_non_varroa_val)\n",
    "print(\"Total number of files = \",number_label_varroa_val+number_label_non_varroa_val)\n",
    "img_list_prep_train = [names for names in img_list_dir_prep if names.endswith(\".jpg\")]\n",
    "img_list_prep_test = [names for names in img_list_dir_prep_test if names.endswith(\".jpg\")]\n",
    "img_list_prep_validation = [names for names in img_list_dir_prep_validation if names.endswith(\".jpg\")]\n",
    "num_imgs_train=number_label_varroa_train+number_label_non_varroa_train\n",
    "num_imgs_test=number_label_varroa_test+number_label_non_varroa_test\n",
    "num_imgs_validation=number_label_varroa_val+number_label_non_varroa_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the training feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_simple=np.zeros((len(img_list_dir_prep),10))\n",
    "kernelSize=4\n",
    "kernel = np.ones((kernelSize,kernelSize))\n",
    "\n",
    "for i in range(len(img_list_dir_prep)):\n",
    "    name=img_list_dir_prep[i]\n",
    "    #if i==0 : #print(name)\n",
    "    img = cv2.imread(prepared_data_path+name)\n",
    "    X_train_simple[i,7]=np.mean(img[:,:,0]/255)\n",
    "    X_train_simple[i,8]=np.mean(img[:,:,1]/255) \n",
    "    X_train_simple[i,9]=np.mean(img[:,:,2]/255) \n",
    "    im= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    X_train_simple[i,4]=np.max(cv2.matchTemplate(im,varroa_gray,cv2.TM_CCOEFF_NORMED))\n",
    "    ret,thresh = cv2.threshold(im,100,255,cv2.THRESH_BINARY)    \n",
    "    thresh[thresh>0]=1\n",
    "    \n",
    "    hough_res=transform.hough_circle(thresh,np.arange(6,8,1))\n",
    "    accum,cx,cy,radii=transform.hough_circle_peaks(hough_res,np.arange(6,8,1),total_num_peaks=10)\n",
    "    X_train_simple[i,5]=hough_res.mean()\n",
    "    X_train_simple[i,6]=hough_res.std() \n",
    "    #if(np.sum(thresh>0)>2100):\n",
    "    #    thresh=np.zeros_like(thresh)\n",
    "    image_proprieties = measure.regionprops(thresh)\n",
    "    for prop in image_proprieties: \n",
    "        X_train_simple[i,0]=np.max(prop.area)\n",
    "        X_train_simple[i,1]=np.max(prop.perimeter)\n",
    "        X_train_simple[i,2]=np.max(prop.perimeter**2/prop.area)\n",
    "        X_train_simple[i,3]=np.max(prop.area/prop.bbox_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the training feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_simple=np.zeros((len(img_list_dir_prep_test),10))\n",
    "kernelSize=4\n",
    "kernel = np.ones((kernelSize,kernelSize))\n",
    "\n",
    "for i in range(len(img_list_dir_prep_test)):\n",
    "    name=img_list_dir_prep_test[i]\n",
    "    #if i==0 : #print(name)\n",
    "    img = cv2.imread(prepared_data_path_test+name)\n",
    "    X_test_simple[i,7]=np.mean(img[:,:,0]/255)\n",
    "    X_test_simple[i,8]=np.mean(img[:,:,1]/255) \n",
    "    X_test_simple[i,9]=np.mean(img[:,:,2]/255)\n",
    "    im= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    X_test_simple[i,4]=np.max(cv2.matchTemplate(im,varroa_gray,cv2.TM_CCOEFF_NORMED))\n",
    "\n",
    "    ret,thresh = cv2.threshold(im,100,255,cv2.THRESH_BINARY)    \n",
    "    thresh[thresh>0]=1  \n",
    "    hough_res=transform.hough_circle(thresh,np.arange(6,8,1))\n",
    "    accum,cx,cy,radii=transform.hough_circle_peaks(hough_res,np.arange(6,8,1),total_num_peaks=10)\n",
    "    X_test_simple[i,5]=hough_res.mean()\n",
    "    X_test_simple[i,6]=hough_res.std() \n",
    "    #if(np.sum(thresh>0)>2100):\n",
    "    #   thresh=np.zeros_like(thresh)\n",
    " \n",
    "    image_proprieties = measure.regionprops(thresh)\n",
    "    for prop in image_proprieties: \n",
    "        X_test_simple[i,0]=np.max(prop.area)\n",
    "        X_test_simple[i,1]=np.max(prop.perimeter)\n",
    "        X_test_simple[i,2]=np.max(prop.perimeter**2/prop.area)\n",
    "        X_test_simple[i,3]=np.max(prop.area/prop.bbox_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the validation feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_simple=np.zeros((len(img_list_prep_validation),10))\n",
    "kernelSize=4\n",
    "kernel = np.ones((kernelSize,kernelSize))\n",
    "\n",
    "for i in range(len(img_list_prep_validation)):\n",
    "    name=img_list_prep_validation[i]\n",
    "    #if i==0 : #print(name)\n",
    "    img = cv2.imread(prepared_data_path_validation+name)\n",
    "    X_val_simple[i,7]=np.mean(img[:,:,0]/255)\n",
    "    X_val_simple[i,8]=np.mean(img[:,:,1]/255) \n",
    "    X_val_simple[i,9]=np.mean(img[:,:,2]/255)\n",
    "    im= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    X_val_simple[i,4]=np.max(cv2.matchTemplate(im,varroa_gray,cv2.TM_CCOEFF_NORMED))\n",
    "\n",
    "    ret,thresh = cv2.threshold(im,100,255,cv2.THRESH_BINARY)    \n",
    "    thresh[thresh>0]=1  \n",
    "    hough_res=transform.hough_circle(thresh,np.arange(6,8,1))\n",
    "    accum,cx,cy,radii=transform.hough_circle_peaks(hough_res,np.arange(6,8,1),total_num_peaks=10)\n",
    "    X_val_simple[i,5]=hough_res.mean()\n",
    "    X_val_simple[i,6]=hough_res.std() \n",
    "    #if(np.sum(thresh>0)>2100):\n",
    "    #   thresh=np.zeros_like(thresh)\n",
    " \n",
    "    image_proprieties = measure.regionprops(thresh)\n",
    "    for prop in image_proprieties: \n",
    "        X_val_simple[i,0]=np.max(prop.area)\n",
    "        X_val_simple[i,1]=np.max(prop.perimeter)\n",
    "        X_val_simple[i,2]=np.max(prop.perimeter**2/prop.area)\n",
    "        X_val_simple[i,3]=np.max(prop.area/prop.bbox_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocessing: Normalizing the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between two types of normalization : \n",
    "* Substracting the mean and dividing by the standard deviation\n",
    "* Normalize in a range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train_simple = preprocessing.scale(X_train_simple)\\nX_test_simple=preprocessing.scale(X_test_simple)\\nX_val_simple=preprocessing.scale(X_val_simple)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train_simple = preprocessing.scale(X_train_simple)\n",
    "X_test_simple=preprocessing.scale(X_test_simple)\n",
    "X_val_simple=preprocessing.scale(X_val_simple)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaler = min_max_scaler.fit(X_train_simple)\n",
    "X_train_simple = scaler.transform(X_train_simple)\n",
    "X_test_simple = scaler.transform(X_test_simple)\n",
    "X_val_simple = scaler.transform(X_val_simple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the label arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_simple=np.zeros((num_imgs_train,1))\n",
    "for i in range(len(img_list_prep_train)):\n",
    "    name=img_list_prep_train[i]\n",
    "    if name.endswith(\"_1.jpg\"):\n",
    "        Y_train_simple[i,0]=1\n",
    "\n",
    "Y_test_simple=np.zeros((num_imgs_test,1))\n",
    "for i in range(len(img_list_prep_test)):\n",
    "    name=img_list_prep_test[i]\n",
    "    if name.endswith(\"_1.jpg\"):\n",
    "        Y_test_simple[i,0]=1\n",
    "\n",
    "Y_val_simple=np.zeros((num_imgs_validation,1))\n",
    "for i in range(len(img_list_prep_validation)):\n",
    "    name=img_list_prep_validation[i]\n",
    "    if name.endswith(\"_1.jpg\"):\n",
    "        Y_val_simple[i,0]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and hyperparameter Tuning using  grid-search on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "({'C': 10, 'gamma': 0.01}, 0.9376193507320177)\n",
      "({'C': 0.1, 'gamma': 0.1}, 0.9361970941250789)\n",
      "({'C': 50, 'gamma': 0.01}, 0.9361283982967573)\n",
      "({'C': 1, 'gamma': 0.1}, 0.9360597208698475)\n",
      "({'C': 50, 'gamma': 0.001}, 0.93539237314844)\n",
      "({'C': 0.01, 'gamma': 1}, 0.9338471419396275)\n",
      "({'C': 50, 'gamma': 1}, 0.9337329337329336)\n",
      "({'C': 1, 'gamma': 0.01}, 0.9324111914492298)\n",
      "({'C': 10, 'gamma': 0.001}, 0.9321608040201005)\n",
      "({'C': 1, 'gamma': 0.001}, 0.9295418641390205)\n",
      "({'C': 0.01, 'gamma': 0.1}, 0.9292482627921668)\n",
      "({'C': 0.1, 'gamma': 0.01}, 0.9286616161616161)\n",
      "({'C': 0.1, 'gamma': 1}, 0.9278830176138252)\n",
      "({'C': 10, 'gamma': 1}, 0.9229202037351444)\n",
      "({'C': 0.001, 'gamma': 1}, 0.9193857965451057)\n",
      "({'C': 10, 'gamma': 0.1}, 0.9144220934197068)\n",
      "({'C': 50, 'gamma': 0.1}, 0.9087779690189328)\n",
      "({'C': 1, 'gamma': 1}, 0.9056994818652849)\n",
      "({'C': 0.1, 'gamma': 0.001}, 0.0)\n",
      "({'C': 0.01, 'gamma': 0.01}, 0.0)\n",
      "({'C': 0.01, 'gamma': 0.001}, 0.0)\n",
      "({'C': 0.001, 'gamma': 0.1}, 0.0)\n",
      "({'C': 0.001, 'gamma': 0.01}, 0.0)\n",
      "({'C': 0.001, 'gamma': 0.001}, 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hypopt import GridSearch\n",
    "Cs = [0.001, 0.01, 0.1, 1,10,50]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "warnings.filterwarnings('ignore')\n",
    "opt = GridSearch(SVC(kernel='rbf'),param_grid)\n",
    "opt.fit(X_train_simple, Y_train_simple, X_val_simple, Y_val_simple, scoring='f1')\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(opt.best_params)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for param in (opt.param_scores):\n",
    "    print(param)\n",
    "print()\n",
    "#print('Test Score for Optimized Parameters:', opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now see the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First , the result on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall=  0.9029761110369678\n",
      "precision=  0.8654387311332822\n",
      "F-measure=  0.8838090261903206\n",
      "Accuracy=  0.9097229270273013\n"
     ]
    }
   ],
   "source": [
    "model_svm=SVC(C=10, kernel='rbf', gamma=0.01)\n",
    "model_svm.fit(X_train_simple, Y_train_simple.ravel())\n",
    "y_pred_svm_content_train=model_svm.predict(X_train_simple)\n",
    "evaluate_method(Y_train_simple.ravel(),y_pred_svm_content_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now on testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall=  0.8676975945017182\n",
      "precision=  0.9692898272552783\n",
      "F-measure=  0.915684496826836\n",
      "Accuracy=  0.9201030927835051\n"
     ]
    }
   ],
   "source": [
    "model_svm.fit(X_train_simple, Y_train_simple.ravel())\n",
    "y_pred_svm_content_test=model_svm.predict(X_test_simple)\n",
    "evaluate_method(Y_test_simple,y_pred_svm_content_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can deduce that there is not a risk of overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPR=  0.9072016542335319\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prec_recall_curve(Y_test_simple,y_pred_svm_content_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc= 0.9201030927835052\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(Y_test_simple,y_pred_svm_content_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see , our model is robust with only 4 features but for only one object by image (after splitting). \n",
    "Let's see now if we can detect varroas on all the testing images without knowing the ground truth using sliding window-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test images as unseen (no ground_truth) : Sliding window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 measure for image 0 is  0.0\n",
      "f1 measure for image 1 is  0.7287802810328069\n",
      "f1 measure for image 2 is  0.0\n",
      "f1 measure for image 3 is  0.8395604395604396\n",
      "f1 measure for image 4 is  0.9714285714285713\n",
      "f1 measure for image 5 is  0.0\n",
      "f1 measure for image 6 is  0.2637144837144837\n",
      "f1 measure for image 7 is  0.41025641025641024\n",
      "f1 measure for image 8 is  0.3998850574712644\n",
      "f1 measure for image 9 is  0.3407439242255705\n",
      "f1 measure for image 10 is  0.6539787798408488\n",
      "f1 measure for image 11 is  0.5030537634408603\n",
      "f1 measure for image 12 is  0.0\n",
      "f1 measure for image 13 is  0.491053146061635\n",
      "f1 measure for image 14 is  0.08\n",
      "f1 measure for image 15 is  0.13333333333333336\n",
      "f1 measure for image 16 is  0.0\n",
      "f1 measure for image 17 is  0.0\n",
      "f1 measure for image 18 is  0.15103485104073766\n",
      "f1 measure for image 19 is  0.22222222222222224\n",
      "f1 measure for image 20 is  0.17605953726215343\n",
      "f1 measure for image 21 is  0.1638095238095238\n",
      "f1 measure for image 22 is  0.0\n",
      "f1 measure for image 23 is  0.3276199545711741\n",
      "f1 measure for image 24 is  0.0\n",
      "f1 measure for image 25 is  1.0\n",
      "f1 measure for image 26 is  0.0\n",
      "f1 measure for image 27 is  1.0\n",
      "f1 measure for image 28 is  0.0\n",
      "f1 measure for image 29 is  1.0\n",
      "f1 measure for image 30 is  0.0\n",
      "f1 measure for image 31 is  0.039999999999999994\n",
      "f1 measure for image 32 is  0.42074217536238445\n",
      "f1 measure for image 33 is  0.09355742296918768\n",
      "f1 measure for image 34 is  0.0\n",
      "f1 measure for image 35 is  0.0\n",
      "f1 measure for image 36 is  0.12820512820512822\n",
      "f1 measure for image 37 is  0.08888888888888889\n",
      "f1 measure for image 38 is  0.08\n",
      "f1 measure for image 39 is  0.0\n",
      "f1 measure for image 40 is  0.03192389006342494\n",
      "f1 measure for image 41 is  0.0\n",
      "f1 measure for image 42 is  0.044244417517742766\n",
      "f1 measure for image 43 is  0.18823529411764708\n",
      "f1 measure for image 44 is  0.0\n",
      "f1 measure for image 45 is  0.0\n",
      "f1 measure for image 46 is  0.0\n",
      "f1 measure for image 47 is  0.0\n",
      "f1 measure for image 48 is  0.13126934984520122\n",
      "f1 measure for image 49 is  0.11818181818181817\n",
      "MEAN F MEASURE 0.22443565328846915\n"
     ]
    }
   ],
   "source": [
    "stepSize=24\n",
    "winW=48\n",
    "winH=48\n",
    "windowSize=(winW,winH)\n",
    "f1_final=[]\n",
    "h=[]\n",
    "hedi=[]\n",
    "X_unseen_simple=np.zeros((1,10))\n",
    "\n",
    "for image_number in range(50):\n",
    "    img_name=img_list_test[image_number]\n",
    "    ID=img_name[:-4]\n",
    "    im=cv2.imread(src_path_test+img_name)\n",
    "    detections = []\n",
    "    ground_truth_1=[]\n",
    "    for i in range(len(annotations_xmls_test[image_number])): \n",
    "        ground_truth_1.append(annotations_xmls_test[image_number][i]['bbox'])\n",
    "    for (x,y,window) in sliding_window(im, stepSize, windowSize=(winW,winH)):\n",
    "            # if the window does not meet our desired window size, ignore it!\n",
    "        if window.shape[0] != winH or window.shape[1] !=winW: # ensure the sliding window has met the minimum size requirement\n",
    "            continue\n",
    "        img=(window)\n",
    "        blue=np.mean(img[:,:,0]/255)\n",
    "        green=np.mean(img[:,:,1]/255) \n",
    "        red=np.mean(img[:,:,2]/255)\n",
    "        img= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret,thresh = cv2.threshold(img,100,255,cv2.THRESH_BINARY)  \n",
    "        tempmatch=np.abs(np.max(cv2.matchTemplate(img,varroa_gray,cv2.TM_CCOEFF_NORMED)))\n",
    "        h.append(tempmatch)\n",
    "        thresh[thresh>0]=1 \n",
    "        hough_res=transform.hough_circle(thresh,np.arange(6,8,1))\n",
    "        accum,cx,cy,radii=transform.hough_circle_peaks(hough_res,np.arange(6,8,1),total_num_peaks=10)\n",
    "        mean=hough_res.mean()\n",
    "        standard=hough_res.std() \n",
    "        image_proprieties = measure.regionprops(thresh)\n",
    "        for prop in image_proprieties: \n",
    "            area=np.max(prop.area)\n",
    "            perimeter=np.max(prop.perimeter) \n",
    "            circularity=np.max(prop.perimeter**2/prop.area)\n",
    "            rectangularity=np.max(prop.area/prop.bbox_area)\n",
    "        X_unseen_simple[0,:]=[area,perimeter,circularity,rectangularity,tempmatch,mean,standard,blue,green,red]\n",
    "       \n",
    "        X_unseen_simple = scaler.transform(X_unseen_simple)\n",
    "        #print(X_unseen_simple.sum())\n",
    "        pred_unseen=model_svm.predict(X_unseen_simple)\n",
    "        hedi.append(pred_unseen)\n",
    "        if pred_unseen == 1:\n",
    "            if model_svm.decision_function(X_unseen_simple) > 0.8:  \n",
    "                detections.append((x , y, model_svm.decision_function(X_unseen_simple),windowSize[0],windowSize[1]))\n",
    "\n",
    "    rects = np.array([[x, y, x + w, y + h] for (x, y, _, w, h) in detections]) # do nms on the detected bounding boxes\n",
    "    sc = [score[0] for (x, y, score, w, h) in detections]\n",
    "    #print(\"detection confidence score: \", sc)\n",
    "    sc = np.array(sc)\n",
    "    pick = non_max_suppression(rects, probs = sc, overlapThresh = 0.1)\n",
    "    if len(pick)!=0:\n",
    "        pick[:,2]=48\n",
    "        pick[:,3]=48\n",
    "    if len(rects)!=0:\n",
    "        rects[:,2]=48\n",
    "        rects[:,3]=48\n",
    "    f1=compute_f1(rects,ground_truth_1,0)\n",
    "    print(\"f1 measure for image {} is \".format(image_number),f1)\n",
    "    f1_final.append(f1)\n",
    "print(\"MEAN F MEASURE\" , np.mean(f1_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function bellow take an image as input and give the precited varroas as output using sliding window on svm model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_method_1(im,model_svm,overlap_thresh_nms,prob_thresh,stepSize):\n",
    "    \"\"\"\n",
    "    This function takes as an imput and perform the model prediction using sliding windows \n",
    "    ---------------\n",
    "    INPUTS : \n",
    "        im: image to predict \n",
    "        model_svm : SVM model used to predict the varroas \n",
    "        overlap_thresh : threshold for the non maximum supression algorithm \n",
    "        StepSize: Step size of the sliding window \n",
    "    OUTPUTS :\n",
    "        pred_varroas : list with the predected varroas after passing through the non maximum supression algorithm\n",
    "    \"\"\"\n",
    "    winW=48\n",
    "    winH=48\n",
    "    windowSize=(winW,winH)\n",
    "    f1_final=[]\n",
    "\n",
    "    X_unseen_simple=np.zeros((1,10))\n",
    "\n",
    "\n",
    "    for (x,y,window) in sliding_window(im, stepSize, windowSize=(winW,winH)):\n",
    "            # if the window does not meet our desired window size, ignore it!\n",
    "        if window.shape[0] != winH or window.shape[1] !=winW: # ensure the sliding window has met the minimum size requirement\n",
    "            continue\n",
    "        img=(window)\n",
    "        blue=np.mean(img[:,:,0]/255)\n",
    "        green=np.mean(img[:,:,1]/255) \n",
    "        red=np.mean(img[:,:,2]/255)\n",
    "        img= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret,thresh = cv2.threshold(img,100,255,cv2.THRESH_BINARY)  \n",
    "        tempmatch=np.abs(np.max(cv2.matchTemplate(img,varroa_gray,cv2.TM_CCOEFF_NORMED)))\n",
    "        h.append(tempmatch)\n",
    "        thresh[thresh>0]=1 \n",
    "        hough_res=transform.hough_circle(thresh,np.arange(6,8,1))\n",
    "        accum,cx,cy,radii=transform.hough_circle_peaks(hough_res,np.arange(6,8,1),total_num_peaks=10)\n",
    "        mean=hough_res.mean()\n",
    "        standard=hough_res.std() \n",
    "        image_proprieties = measure.regionprops(thresh)\n",
    "        for prop in image_proprieties: \n",
    "            area=np.max(prop.area)\n",
    "            perimeter=np.max(prop.perimeter) \n",
    "            circularity=np.max(prop.perimeter**2/prop.area)\n",
    "            rectangularity=np.max(prop.area/prop.bbox_area)\n",
    "        X_unseen_simple[0,:]=[area,perimeter,circularity,rectangularity,tempmatch,mean,standard,blue,green,red]\n",
    "\n",
    "        X_unseen_simple = scaler.transform(X_unseen_simple)\n",
    "        #print(X_unseen_simple.sum())\n",
    "        pred_unseen=model_svm.predict(X_unseen_simple)\n",
    "        hedi.append(pred_unseen)\n",
    "        if pred_unseen == 1:\n",
    "            if model_svm.decision_function(X_unseen_simple) > prob_thresh:  \n",
    "                detections.append((x , y, model_svm.decision_function(X_unseen_simple),windowSize[0],windowSize[1]))\n",
    "\n",
    "    rects = np.array([[x, y, x + w, y + h] for (x, y, _, w, h) in detections]) # do nms on the detected bounding boxes\n",
    "    sc = [score[0] for (x, y, score, w, h) in detections]\n",
    "    #print(\"detection confidence score: \", sc)\n",
    "    sc = np.array(sc)\n",
    "    pick = non_max_suppression(rects, probs = sc, overlapThresh = overlap_thresh_nms)\n",
    "    if len(pick)!=0:\n",
    "        pick[:,2]=48\n",
    "        pick[:,3]=48\n",
    "    if len(rects)!=0:\n",
    "        rects[:,2]=48\n",
    "        rects[:,3]=48\n",
    "    predicted_varroas=pick\n",
    "    \n",
    "    return predicted_varroas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM-HOG DETECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hedi Fendri\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "X_train_simple_hog=np.zeros((len(img_list_dir_prep),1296))\n",
    "\n",
    "for i in range(len(img_list_dir_prep)):\n",
    "    name=img_list_dir_prep[i]\n",
    "    #if i==0 : #print(name)\n",
    "    img = cv2.imread(prepared_data_path+name)\n",
    "    hog_feature=hog(img)\n",
    "    hog_feature=hog_feature.reshape(1,-1)\n",
    "    X_train_simple_hog[i]=hog_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hedi Fendri\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "X_val_simple_hog=np.zeros((len(img_list_dir_prep_validation),1296))\n",
    "\n",
    "for i in range(len(img_list_dir_prep_validation)):\n",
    "    name=img_list_dir_prep_validation[i]\n",
    "    #if i==0 : #print(name)\n",
    "    img = cv2.imread(prepared_data_path_validation+name)\n",
    "    hog_feature=hog(img)\n",
    "    hog_feature=hog_feature.reshape(1,-1)\n",
    "    X_val_simple_hog[i]=hog_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hedi Fendri\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "X_test_simple_hog=np.zeros((len(img_list_prep_test),1296))\n",
    "for i in range(len(img_list_prep_test)):\n",
    "    name=img_list_dir_prep_test[i]\n",
    "    #if i==0 : #print(name)\n",
    "    img = cv2.imread(prepared_data_path_test+name)\n",
    "    hog_feature=hog(img)\n",
    "    hog_feature=hog_feature.reshape(1,-1)\n",
    "    X_test_simple_hog[i]=hog_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 50, 'gamma': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "({'C': 50, 'gamma': 1}, 0.9539582643259358)\n",
      "({'C': 10, 'gamma': 1}, 0.953519256308101)\n",
      "({'C': 1, 'gamma': 1}, 0.9414141414141414)\n",
      "({'C': 50, 'gamma': 0.1}, 0.9391187352842247)\n",
      "({'C': 10, 'gamma': 0.1}, 0.9275756545392724)\n",
      "({'C': 50, 'gamma': 0.01}, 0.9118457300275482)\n",
      "({'C': 0.1, 'gamma': 1}, 0.8990182328190743)\n",
      "({'C': 1, 'gamma': 0.1}, 0.893258426966292)\n",
      "({'C': 10, 'gamma': 0.01}, 0.8920154766092155)\n",
      "({'C': 50, 'gamma': 0.001}, 0.8707824838478105)\n",
      "({'C': 0.1, 'gamma': 0.1}, 0.7975318164288469)\n",
      "({'C': 1, 'gamma': 0.01}, 0.7956740054074932)\n",
      "({'C': 10, 'gamma': 0.001}, 0.794276875483372)\n",
      "({'C': 0.01, 'gamma': 1}, 0.7797408716136632)\n",
      "({'C': 1, 'gamma': 0.001}, 0.43111337061515004)\n",
      "({'C': 0.1, 'gamma': 0.01}, 0.4287169042769857)\n",
      "({'C': 0.01, 'gamma': 0.1}, 0.37295298468040144)\n",
      "({'C': 0.001, 'gamma': 1}, 0.0)\n",
      "({'C': 0.001, 'gamma': 0.01}, 0.0)\n",
      "({'C': 0.001, 'gamma': 0.1}, 0.0)\n",
      "({'C': 0.1, 'gamma': 0.001}, 0.0)\n",
      "({'C': 0.01, 'gamma': 0.001}, 0.0)\n",
      "({'C': 0.01, 'gamma': 0.01}, 0.0)\n",
      "({'C': 0.001, 'gamma': 0.001}, 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hypopt import GridSearch\n",
    "Cs = [0.001, 0.01, 0.1, 1,10,50]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "warnings.filterwarnings('ignore')\n",
    "opt = GridSearch(SVC(kernel='rbf'),param_grid)\n",
    "opt.fit(X_train_simple_hog, Y_train_simple, X_val_simple_hog, Y_val_simple, scoring='f1')\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(opt.best_params)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for param in (opt.param_scores):\n",
    "    print(param)\n",
    "print()\n",
    "#print('Test Score for Optimized Parameters:', opt.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing The SVM_based on HOG method : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm=SVC(C=50, kernel='rbf', gamma=1)\n",
    "model_svm.fit(X_train_simple_hog, Y_train_simple.ravel())\n",
    "y_pred_svm_content_test_hog=model_svm.predict(X_test_simple_hog)\n",
    "evaluate_method(Y_test_simple.ravel(),y_pred_svm_content_test_hog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepSize=14\n",
    "winW=48\n",
    "winH=48\n",
    "windowSize=(winW,winH)\n",
    "f1_final=[]\n",
    "jason_hog={}\n",
    "jason_hog_nms={}\n",
    "for image_number in range(50):\n",
    "    img_name=img_list_test[image_number]\n",
    "    ID=img_name[:-4]\n",
    "    im=cv2.imread(src_path_test+img_name)\n",
    "    detections = []\n",
    "    ground_truth_1=[]\n",
    "    for i in range(len(annotations_xmls_test[image_number])): \n",
    "        ground_truth_1.append(annotations_xmls_test[image_number][i]['bbox'])\n",
    "    for (x,y,window) in sliding_window(im, stepSize, windowSize=(winW,winH)):\n",
    "            # if the window does not meet our desired window size, ignore it!\n",
    "            if window.shape[0] != winH or window.shape[1] !=winW: # ensure the sliding window has met the minimum size requirement\n",
    "                continue\n",
    "            window=rgb2gray(window)\n",
    "            fds = hog(window) # extract HOG features from the window captured\n",
    "            fds = fds.reshape(1, -1) # re shape the image to make a silouhette of hog\n",
    "            pred = model_svm.predict(fds) # use the SVM model to make a prediction on the HOG features extracted from the window\n",
    "\n",
    "            if pred == 1:\n",
    "                if model_svm.decision_function(fds) > 0.5:  # set a threshold value for the SVM prediction i.e. only firm the predictions above probability of 0.6\n",
    "                    #print(\"Detection:: Location -> ({}, {})\".format(x, y))\n",
    "                    #print(\" | Confidence Score {} \\n\".format(model_svm.decision_function(fds)))\n",
    "                    detections.append((x , y, model_svm.decision_function(fds),windowSize[0],windowSize[1]))\n",
    "\n",
    "    rects = np.array([[x, y, x + w, y + h] for (x, y, _, w, h) in detections]) # do nms on the detected bounding boxes\n",
    "    sc = [score[0] for (x, y, score, w, h) in detections]\n",
    "    #print(\"detection confidence score: \", sc)\n",
    "    sc = np.array(sc)\n",
    "    pick = non_max_suppression(rects, probs = sc, overlapThresh = 0.1)\n",
    "    if len(pick)!=0:\n",
    "        pick[:,2]=48\n",
    "        pick[:,3]=48\n",
    "    if len(rects)!=0:\n",
    "        rects[:,2]=48\n",
    "        rects[:,3]=48\n",
    "    f1=compute_f1(pick,ground_truth_1,0)\n",
    "    print(\"f1 measure for image {} is \".format(image_number),f1)\n",
    "    f1_final.append(f1)\n",
    "    jason_hog_nms[ID]=pick\n",
    "    jason_hog[ID]=rects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of F_measure using SVM based on HOG is slightely better : 0.20 on testing as seen in the cell bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 measure for image 0 is  0.0\n",
      "f1 measure for image 1 is  0.7770921897483739\n",
      "f1 measure for image 2 is  0.13793103448275862\n",
      "f1 measure for image 3 is  0.74\n",
      "f1 measure for image 4 is  0.5636438923395445\n",
      "f1 measure for image 5 is  0.0\n",
      "f1 measure for image 6 is  0.1115187594401906\n",
      "f1 measure for image 7 is  0.5185948310948311\n",
      "f1 measure for image 8 is  0.4697810179179803\n",
      "f1 measure for image 9 is  0.14753728959198928\n",
      "f1 measure for image 10 is  0.18971841714925752\n",
      "f1 measure for image 11 is  0.5632275341480948\n",
      "f1 measure for image 12 is  0.05647058823529412\n",
      "f1 measure for image 13 is  0.537324583531903\n",
      "f1 measure for image 14 is  0.07486157253599114\n",
      "f1 measure for image 15 is  0.15805194805194805\n",
      "f1 measure for image 16 is  0.0\n",
      "f1 measure for image 17 is  0.1\n",
      "f1 measure for image 18 is  0.15977492157252565\n",
      "f1 measure for image 19 is  0.16313171572658433\n",
      "f1 measure for image 20 is  0.2163937193644485\n",
      "f1 measure for image 21 is  0.4207844464558194\n",
      "f1 measure for image 22 is  0.0\n",
      "f1 measure for image 23 is  0.3485071793851766\n",
      "f1 measure for image 24 is  0.0\n",
      "f1 measure for image 25 is  1.0\n",
      "f1 measure for image 26 is  0.0\n",
      "f1 measure for image 27 is  1.0\n",
      "f1 measure for image 28 is  0.0\n",
      "f1 measure for image 29 is  0.0\n",
      "f1 measure for image 30 is  0.0\n",
      "f1 measure for image 31 is  0.09338731443994602\n",
      "f1 measure for image 32 is  0.4299601287061637\n",
      "f1 measure for image 33 is  0.12443794909268054\n",
      "f1 measure for image 34 is  0.0\n",
      "f1 measure for image 35 is  0.0\n",
      "f1 measure for image 36 is  0.1192982456140351\n",
      "f1 measure for image 37 is  0.14725490196078433\n",
      "f1 measure for image 38 is  0.06666666666666668\n",
      "f1 measure for image 39 is  0.0\n",
      "f1 measure for image 40 is  0.050468902824923764\n",
      "f1 measure for image 41 is  0.0\n",
      "f1 measure for image 42 is  0.13730460545754183\n",
      "f1 measure for image 43 is  0.2041431261770245\n",
      "f1 measure for image 44 is  0.0\n",
      "f1 measure for image 45 is  0.0\n",
      "f1 measure for image 46 is  0.0\n",
      "f1 measure for image 47 is  0.0\n",
      "f1 measure for image 48 is  0.12201369551302514\n",
      "f1 measure for image 49 is  0.0710814682948539\n",
      "Fmean = 0.20040725291040712\n"
     ]
    }
   ],
   "source": [
    "fmean=[]\n",
    "for i in range(50):\n",
    "    \n",
    "    im_name=img_list_test[i]\n",
    "    ground_truth_1=[]\n",
    "    for j in range(len(annotations_xmls_test[i])): \n",
    "        ground_truth_1.append(annotations_xmls_test[i][j]['bbox'])\n",
    "    ID=im_name[:-4]\n",
    "    box=jason_hog[ID]\n",
    "    f1=compute_f1(box,ground_truth_1)\n",
    "    print(\"f1 measure for image {} is \".format(i),f1)\n",
    "    fmean.append(f1)\n",
    "print(\"Fmean =\",np.mean(fmean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function performs the SVM-HOG using sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_method_1_hog(im,model_svm,overlap_thresh_nms,prob_thresh,stepSize):\n",
    "    \"\"\"\n",
    "    This function takes as an imput and perform the model prediction using sliding windows \n",
    "    ---------------\n",
    "    INPUTS : \n",
    "        im: image to predict \n",
    "        model_svm : SVM model used to predict the varroas \n",
    "        overlap_thresh : threshold for the non maximum supression algorithm \n",
    "        StepSize: Step size of the sliding window \n",
    "    OUTPUTS :\n",
    "        pred_varroas : list with the predected varroas after passing through the non maximum supression algorithm\n",
    "    \"\"\"\n",
    "    winW=48\n",
    "    winH=48\n",
    "    windowSize=(winW,winH)\n",
    "    f1_final=[]\n",
    "\n",
    "    X_unseen_simple=np.zeros((1,10))\n",
    "    for (x,y,window) in sliding_window(im, stepSize, windowSize=(winW,winH)):\n",
    "            # if the window does not meet our desired window size, ignore it!\n",
    "            if window.shape[0] != winH or window.shape[1] !=winW: # ensure the sliding window has met the minimum size requirement\n",
    "                continue\n",
    "            window=rgb2gray(window)\n",
    "            fds = hog(window) # extract HOG features from the window captured\n",
    "            fds = fds.reshape(1, -1) # re shape the image to make a silouhette of hog\n",
    "            pred = model_svm.predict(fds) # use the SVM model to make a prediction on the HOG features extracted from the window\n",
    "\n",
    "            if pred == 1:\n",
    "                if model_svm.decision_function(fds) > 0.5:  # set a threshold value for the SVM prediction i.e. only firm the predictions above probability of 0.6\n",
    "                    #print(\"Detection:: Location -> ({}, {})\".format(x, y))\n",
    "                    #print(\" | Confidence Score {} \\n\".format(model_svm.decision_function(fds)))\n",
    "                    detections.append((x , y, model_svm.decision_function(fds),windowSize[0],windowSize[1]))\n",
    "\n",
    "    rects = np.array([[x, y, x + w, y + h] for (x, y, _, w, h) in detections]) # do nms on the detected bounding boxes\n",
    "    sc = [score[0] for (x, y, score, w, h) in detections]\n",
    "    #print(\"detection confidence score: \", sc)\n",
    "    sc = np.array(sc)\n",
    "    pick = non_max_suppression(rects, probs = sc, overlapThresh = 0.1)\n",
    "    if len(pick)!=0:\n",
    "        pick[:,2]=48\n",
    "        pick[:,3]=48\n",
    "    if len(rects)!=0:\n",
    "        rects[:,2]=48\n",
    "        rects[:,3]=48\n",
    "    predicted_varroas=pick\n",
    "    return predicted_varroas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using MLP and CNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your implementation for the thrid part. Feel free to add your desirable functions, but please make sure you have proper functions for the final detection, where their input and output follows the same format as the previous parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We comoute in the cell bellow the maximum size for each training , validation and testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builing the training, validation and testing numpy arrays with pixels normaized from 0 to 1 \n",
    "The X_train size is (N,48,48,3) where N is the number of samples for training and 3 represents the rgb colors <br> \n",
    "The Y_train size is (N,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np.zeros((num_imgs_train,1))\n",
    "\n",
    "X_train=np.zeros((num_imgs_train,48,48,3))\n",
    "for i in range(len(img_list_prep_train)):\n",
    "    name=img_list_prep_train[i]\n",
    "    img = cv2.imread(prepared_data_path+name)\n",
    "    X_train[i,:,:,:]=img/255\n",
    "    if name.endswith(\"_1.jpg\"):\n",
    "        Y_train[i,0]=1\n",
    "\n",
    "\n",
    "X_test=np.zeros((num_imgs_test,48,48,3))\n",
    "Y_test=np.zeros((num_imgs_test,1))\n",
    "\n",
    "for i in range(len(img_list_prep_test)):\n",
    "    name=img_list_prep_test[i]\n",
    "    img = cv2.imread(prepared_data_path_test+name)\n",
    "    X_test[i,:,:,:]=img/255\n",
    "    if name.endswith(\"_1.jpg\"):\n",
    "        Y_test[i,0]=1\n",
    "\n",
    "X_val=np.zeros((num_imgs_validation,48,48,3))\n",
    "Y_val=np.zeros((num_imgs_validation,1))\n",
    "\n",
    "for i in range(len(img_list_prep_validation)):\n",
    "    name=img_list_prep_validation[i]\n",
    "    img = cv2.imread(prepared_data_path_validation+name)\n",
    "    X_val[i,:,:,:]=img/255\n",
    "    if name.endswith(\"_1.jpg\"):\n",
    "        Y_val[i,0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19706, 48, 48, 3)\n",
      "(1164, 48, 48, 3)\n",
      "(3078, 48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add the validation to the training , The number of samples is huge and we didin't have enough time to tune our\n",
    "hyperaparameters on the validation dataset for Building The CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.concatenate((X_train, X_val), axis=0)\n",
    "Y_train=np.concatenate((Y_train, Y_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22784, 48, 48, 3)\n",
      "(1164, 48, 48, 3)\n",
      "(3078, 48, 48, 3)\n",
      "(22784, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epoch = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize again our images by substracting the mean and dividing by the standard deviation , this will allow us to have \n",
    "better and faster training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.52077171 0.51788399 0.51323277]]]] [[[[0.22576122 0.20163474 0.1756285 ]]]]\n",
      "[[[[0.61661223 0.60012186 0.58430408]]]] [[[[0.2451306  0.21262974 0.17709751]]]]\n",
      "[[[[0.52079765 0.52045683 0.5194581 ]]]] [[[[0.22291291 0.20315064 0.17051389]]]]\n"
     ]
    }
   ],
   "source": [
    "imgs_mean_train = np.mean(X_train,axis=(0,1,2)).reshape((1,1,1,-1))\n",
    "imgs_std_train = np.std(X_train,axis=(0,1,2)).reshape((1,1,1,-1))\n",
    "print(imgs_mean_train,imgs_std_train)\n",
    "\n",
    "imgs_mean_test = np.mean(X_test,axis=(0,1,2)).reshape((1,1,1,-1))\n",
    "imgs_std_test = np.std(X_test,axis=(0,1,2)).reshape((1,1,1,-1))\n",
    "print(imgs_mean_test,imgs_std_test)\n",
    "\n",
    "imgs_mean_val = np.mean(X_val,axis=(0,1,2)).reshape((1,1,1,-1))\n",
    "imgs_std_val = np.std(X_val,axis=(0,1,2)).reshape((1,1,1,-1))\n",
    "print(imgs_mean_val,imgs_std_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22784, 48, 48, 3) 2.9014625301472252e-11 0.9999999999872363\n",
      "(1164, 48, 48, 3) -5.81028261861697e-11 0.999999999998175\n",
      "(3078, 48, 48, 3) -3.4137015239728173e-11 1.0000000000050686\n"
     ]
    }
   ],
   "source": [
    "X_train = (X_train - imgs_mean_train) / imgs_std_train #.reshape(num_imgs, -1)\n",
    "print(X_train.shape, np.mean(X_train), np.std(X_train))\n",
    "\n",
    "X_test = (X_test - imgs_mean_test) / imgs_std_test #.reshape(num_imgs, -1)\n",
    "print(X_test.shape, np.mean(X_test), np.std(X_test))\n",
    "\n",
    "X_val = (X_val - imgs_mean_val) / imgs_std_val #.reshape(num_imgs, -1)\n",
    "print(X_val.shape, np.mean(X_val), np.std(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=576, out_features=1024, bias=True)\n",
      "  (logit): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define network architecture\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output, n_c):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.logit = torch.nn.Linear(n_hidden, 1)\n",
    "        \n",
    "        self.conv1 = torch.nn.Sequential(         # \n",
    "            torch.nn.Conv2d(\n",
    "                in_channels = n_c,            # input height\n",
    "                out_channels = 16,             # n_filters\n",
    "                kernel_size = 3,              # filter size\n",
    "                stride = 2,                   # filter movement/step\n",
    "                padding = 0,                  \n",
    "            ),                              \n",
    "            torch.nn.ReLU(),                      # activation\n",
    "            #torch.nn.MaxPool2d(kernel_size = 2),    \n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(       \n",
    "            torch.nn.Conv2d(in_channels = 16, \n",
    "                            out_channels = 32, \n",
    "                            kernel_size = 3, \n",
    "                            stride = 2, \n",
    "                            padding = 0),      \n",
    "            torch.nn.ReLU(),                      # activation\n",
    "            #torch.nn.MaxPool2d(2),                \n",
    "        )\n",
    "        \n",
    "        self.conv3 = torch.nn.Sequential(       \n",
    "            torch.nn.Conv2d(in_channels = 32, \n",
    "                            out_channels = 64, \n",
    "                            kernel_size = 3, \n",
    "                            stride = 1, \n",
    "                            padding = 0), \n",
    "               torch.nn.ReLU(),                      # activation\n",
    "            #torch.nn.MaxPool2d(2),                \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.conv1(x)\n",
    "        feat = self.conv2(feat)\n",
    "       \n",
    "        feat = self.conv3(feat)\n",
    "       \n",
    "\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "        \n",
    "        x2 = F.relu(self.hidden(feat))      # activation function for hidden layer\n",
    "       \n",
    "        out_logit = torch.sigmoid(self.logit(x2))\n",
    "        \n",
    "        return out_logit\n",
    "      \n",
    "net = Net(n_feature = 5184, n_hidden = 1024, n_output = 5, n_c = 3)     # define the network\n",
    "print(net)  # net architecture\n",
    "classification_criterion =  torch.nn.BCELoss()# Hint: Consider that we only one class to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CNN Using Binary Cross Entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  Total loss -> 0.38350   classif_loss -> 0.38350  \n",
      "epoch: 1  Total loss -> 0.14126   classif_loss -> 0.14126  \n",
      "epoch: 2  Total loss -> 0.17418   classif_loss -> 0.17418  \n",
      "epoch: 3  Total loss -> 0.08306   classif_loss -> 0.08306  \n",
      "epoch: 4  Total loss -> 0.13552   classif_loss -> 0.13552  \n",
      "epoch: 5  Total loss -> 0.09056   classif_loss -> 0.09056  \n",
      "epoch: 6  Total loss -> 0.07648   classif_loss -> 0.07648  \n",
      "epoch: 7  Total loss -> 0.07422   classif_loss -> 0.07422  \n",
      "epoch: 8  Total loss -> 0.06819   classif_loss -> 0.06819  \n",
      "epoch: 9  Total loss -> 0.06377   classif_loss -> 0.06377  \n"
     ]
    }
   ],
   "source": [
    "# Instanciate the network and define the optimizer\n",
    "num_channels=3\n",
    "net = Net(n_feature = 5184, n_hidden = 1024, n_output = 1, n_c = 3)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "if(X_train.shape[1]!=num_channels): #dim1==channel\n",
    "    X_train = X_train.transpose((0,3,1,2))\n",
    "n_batch = X_train.shape[0]//batch_size\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch in range(n_batch):\n",
    "        batch_X = X_train[batch*batch_size:min((batch+1)*batch_size, X_train.shape[0])]\n",
    "        batch_y = Y_train[batch*batch_size:min((batch+1)*batch_size, X_train.shape[0])]\n",
    "        out_logit = net(torch.tensor(batch_X, dtype=torch.float32))\n",
    "        \n",
    "        mask_arr = np.argwhere(batch_y[:,-1]==1).reshape((-1,))\n",
    "        classification_loss = classification_criterion(out_logit, torch.tensor(batch_y[:,-1:], dtype=torch.float32))\n",
    "        \n",
    "        # Compose the 2 loss functions using the weight gamma (1 line)\n",
    "        loss = classification_loss \n",
    "        \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "    \n",
    "    print('epoch: {}  Total loss -> {:.5f}   classif_loss -> {:.5f}  '\n",
    "          .format(epoch, loss.item(), classification_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(pred_class, ground_truth):\n",
    "    \"\"\"\n",
    "    This function calculate the accuracy and f_measure of the CNN \n",
    "    ------------\n",
    "    Inputs: \n",
    "        pred_class : predicted labels \n",
    "        ground_truth: ground truth \n",
    "    \"\"\"\n",
    "    # classification accuracy\n",
    "    print(\"classification acc: \", np.mean(ground_truth==pred_class))\n",
    "    # fmeasure \n",
    "    print(\"fmeasure : \", metrics.f1_score(ground_truth,pred_class))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating performance on training Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification acc:  0.8649490870786517\n",
      "fmeasure :  0.8515749360860547\n"
     ]
    }
   ],
   "source": [
    "if(X_train.shape[1]!=num_channels):\n",
    "    X_train = X_train.transpose((0,3,1,2))\n",
    "# Predict bounding boxes on the train images.\n",
    "with torch.no_grad():\n",
    "    pred_y_train_logit = net.forward(torch.tensor(X_train, dtype=torch.float32))\n",
    "    pred_y_train_logit = pred_y_train_logit.numpy()\n",
    "    pred_y_train_label = pred_y_train_logit>0.5\n",
    "    \n",
    "\n",
    "calculate_accuracy(pred_y_train_label, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's important to notice that the F-measure is more representative since the dataset in unbalanced, Accuracy will be balanced\n",
    "toward the class that have more samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the performing on testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification acc:  0.9458762886597938\n",
      "fmeasure :  0.9468354430379746\n"
     ]
    }
   ],
   "source": [
    "if(X_test.shape[1]!=num_channels):\n",
    "    X_test = X_test.transpose((0,3,1,2))\n",
    "# Predict bounding boxes on the train images.\n",
    "with torch.no_grad():\n",
    "    pred_y_test_logit = net.forward(torch.tensor(X_test, dtype=torch.float32))\n",
    "    pred_y_test_logit = pred_y_test_logit.numpy()\n",
    "    pred_y_test_label = pred_y_test_logit>0.5\n",
    "    \n",
    "calculate_accuracy(pred_y_test_label, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrestingly, we have a very good result for detecting the varroas when we have only one object in the images (spllited images).<br> What we have to do now is to try to find the varroas on the big images without nowing the regions of interest, in other terms the ground truth locations of the varroas. It's very challenging problem since we are obliged to scan all the image with a corresonding stepSize, this is called sliding_window. <br> We predict here that we will detect correcrly the locations of the varroas (Good TP) but we will detect also a huge number of false postives (FP). That's why we will inject those false postives to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_box(ground_truth,box):\n",
    "    \"\"\"\n",
    "    Comute the iou from a predicted box and all the ground truth boxes of an image \n",
    "    -----------------\n",
    "    Inputs :\n",
    "        ground_truth : list of boxes (eg [[x1,y1,w1,h1],...,[xn,yn,wn,hn]])\n",
    "    Outputs:\n",
    "        iou : numpy array\n",
    "    \"\"\"\n",
    "    iou=np.zeros((len(ground_truth),1))\n",
    "    i=-1\n",
    "    for ground_box in ground_truth:\n",
    "        i+=1\n",
    "        iou[i]=calculate_IOU(ground_box,box)\n",
    "        \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuls of the CNN on all testing images considered as unseen (Without ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 0 is  1.0\n",
      "TP with threshold 0.1 = 287\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 4\n",
      "F_measure for image with threshold=0.1 = 0.9930795847750865\n",
      "TP with threshold 0.2 = 281\n",
      "FP with threshold 0.2 = 6\n",
      "FN with threshold 0.2 = 4\n",
      "F_measure for image with threshold=0.2 = 0.9825174825174825\n",
      "TP with threshold 0.3 = 243\n",
      "FP with threshold 0.3 = 44\n",
      "FN with threshold 0.3 = 4\n",
      "F_measure for image with threshold=0.3 = 0.9101123595505618\n",
      "TP with threshold 0.4 = 196\n",
      "FP with threshold 0.4 = 91\n",
      "FN with threshold 0.4 = 4\n",
      "F_measure for image with threshold=0.4 = 0.8049281314168377\n",
      "TP with threshold 0.5 = 110\n",
      "FP with threshold 0.5 = 177\n",
      "FN with threshold 0.5 = 4\n",
      "F_measure for image with threshold=0.5 = 0.5486284289276808\n",
      "Mean F Measure = 0.8478531974375298\n",
      "f1 measure for image 1 is  0.8478531974375298\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.0\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.0\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.0\n",
      "f1 measure for image 2 is  0.0\n",
      "TP with threshold 0.1 = 4\n",
      "FP with threshold 0.1 = 2\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.7272727272727272\n",
      "TP with threshold 0.2 = 4\n",
      "FP with threshold 0.2 = 2\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.7272727272727272\n",
      "TP with threshold 0.3 = 4\n",
      "FP with threshold 0.3 = 2\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.7272727272727272\n",
      "TP with threshold 0.4 = 3\n",
      "FP with threshold 0.4 = 3\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.6\n",
      "TP with threshold 0.5 = 3\n",
      "FP with threshold 0.5 = 3\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.6\n",
      "Mean F Measure = 0.6763636363636364\n",
      "f1 measure for image 3 is  0.6763636363636364\n",
      "TP with threshold 0.1 = 5\n",
      "FP with threshold 0.1 = 1\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.8333333333333334\n",
      "TP with threshold 0.2 = 5\n",
      "FP with threshold 0.2 = 1\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.8333333333333334\n",
      "TP with threshold 0.3 = 3\n",
      "FP with threshold 0.3 = 3\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.6\n",
      "TP with threshold 0.4 = 3\n",
      "FP with threshold 0.4 = 3\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.6\n",
      "TP with threshold 0.5 = 3\n",
      "FP with threshold 0.5 = 3\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.6\n",
      "Mean F Measure = 0.6933333333333334\n",
      "f1 measure for image 4 is  0.6933333333333334\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 5 is  1.0\n",
      "TP with threshold 0.1 = 8\n",
      "FP with threshold 0.1 = 10\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.5925925925925926\n",
      "TP with threshold 0.2 = 7\n",
      "FP with threshold 0.2 = 11\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.5384615384615385\n",
      "TP with threshold 0.3 = 5\n",
      "FP with threshold 0.3 = 13\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.4166666666666667\n",
      "TP with threshold 0.4 = 3\n",
      "FP with threshold 0.4 = 15\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.27272727272727276\n",
      "TP with threshold 0.5 = 2\n",
      "FP with threshold 0.5 = 16\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.1904761904761905\n",
      "Mean F Measure = 0.4021848521848522\n",
      "f1 measure for image 6 is  0.4021848521848522\n",
      "TP with threshold 0.1 = 8\n",
      "FP with threshold 0.1 = 8\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.64\n",
      "TP with threshold 0.2 = 8\n",
      "FP with threshold 0.2 = 8\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.64\n",
      "TP with threshold 0.3 = 7\n",
      "FP with threshold 0.3 = 9\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.5833333333333334\n",
      "TP with threshold 0.4 = 5\n",
      "FP with threshold 0.4 = 11\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.45454545454545453\n",
      "TP with threshold 0.5 = 2\n",
      "FP with threshold 0.5 = 14\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.21052631578947367\n",
      "Mean F Measure = 0.5056810207336524\n",
      "f1 measure for image 7 is  0.5056810207336524\n",
      "TP with threshold 0.1 = 89\n",
      "FP with threshold 0.1 = 55\n",
      "FN with threshold 0.1 = 2\n",
      "F_measure for image with threshold=0.1 = 0.7574468085106383\n",
      "TP with threshold 0.2 = 78\n",
      "FP with threshold 0.2 = 66\n",
      "FN with threshold 0.2 = 2\n",
      "F_measure for image with threshold=0.2 = 0.6964285714285714\n",
      "TP with threshold 0.3 = 54\n",
      "FP with threshold 0.3 = 90\n",
      "FN with threshold 0.3 = 2\n",
      "F_measure for image with threshold=0.3 = 0.5399999999999999\n",
      "TP with threshold 0.4 = 32\n",
      "FP with threshold 0.4 = 112\n",
      "FN with threshold 0.4 = 2\n",
      "F_measure for image with threshold=0.4 = 0.3595505617977528\n",
      "TP with threshold 0.5 = 10\n",
      "FP with threshold 0.5 = 134\n",
      "FN with threshold 0.5 = 2\n",
      "F_measure for image with threshold=0.5 = 0.12820512820512822\n",
      "Mean F Measure = 0.4963262139884181\n",
      "f1 measure for image 8 is  0.4963262139884181\n",
      "TP with threshold 0.1 = 4\n",
      "FP with threshold 0.1 = 8\n",
      "FN with threshold 0.1 = 2\n",
      "F_measure for image with threshold=0.1 = 0.4444444444444444\n",
      "TP with threshold 0.2 = 4\n",
      "FP with threshold 0.2 = 8\n",
      "FN with threshold 0.2 = 2\n",
      "F_measure for image with threshold=0.2 = 0.4444444444444444\n",
      "TP with threshold 0.3 = 4\n",
      "FP with threshold 0.3 = 8\n",
      "FN with threshold 0.3 = 2\n",
      "F_measure for image with threshold=0.3 = 0.4444444444444444\n",
      "TP with threshold 0.4 = 3\n",
      "FP with threshold 0.4 = 9\n",
      "FN with threshold 0.4 = 2\n",
      "F_measure for image with threshold=0.4 = 0.35294117647058826\n",
      "TP with threshold 0.5 = 1\n",
      "FP with threshold 0.5 = 11\n",
      "FN with threshold 0.5 = 2\n",
      "F_measure for image with threshold=0.5 = 0.13333333333333333\n",
      "Mean F Measure = 0.36392156862745095\n",
      "f1 measure for image 9 is  0.36392156862745095\n",
      "TP with threshold 0.1 = 13\n",
      "FP with threshold 0.1 = 5\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.8125000000000001\n",
      "TP with threshold 0.2 = 12\n",
      "FP with threshold 0.2 = 6\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.7741935483870968\n",
      "TP with threshold 0.3 = 9\n",
      "FP with threshold 0.3 = 9\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.6428571428571429\n",
      "TP with threshold 0.4 = 5\n",
      "FP with threshold 0.4 = 13\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.4166666666666667\n",
      "TP with threshold 0.5 = 3\n",
      "FP with threshold 0.5 = 15\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.27272727272727276\n",
      "Mean F Measure = 0.5837889261276359\n",
      "f1 measure for image 10 is  0.5837889261276359\n",
      "TP with threshold 0.1 = 24\n",
      "FP with threshold 0.1 = 19\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.7058823529411763\n",
      "TP with threshold 0.2 = 21\n",
      "FP with threshold 0.2 = 22\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.6461538461538462\n",
      "TP with threshold 0.3 = 17\n",
      "FP with threshold 0.3 = 26\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.5573770491803278\n",
      "TP with threshold 0.4 = 11\n",
      "FP with threshold 0.4 = 32\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.4\n",
      "TP with threshold 0.5 = 2\n",
      "FP with threshold 0.5 = 41\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.08695652173913045\n",
      "Mean F Measure = 0.47927395400289613\n",
      "f1 measure for image 11 is  0.47927395400289613\n",
      "TP with threshold 0.1 = 20\n",
      "FP with threshold 0.1 = 38\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.5063291139240507\n",
      "TP with threshold 0.2 = 17\n",
      "FP with threshold 0.2 = 41\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.44736842105263147\n",
      "TP with threshold 0.3 = 3\n",
      "FP with threshold 0.3 = 55\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0967741935483871\n",
      "TP with threshold 0.4 = 3\n",
      "FP with threshold 0.4 = 55\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0967741935483871\n",
      "TP with threshold 0.5 = 2\n",
      "FP with threshold 0.5 = 56\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.06557377049180328\n",
      "Mean F Measure = 0.24256393851305189\n",
      "f1 measure for image 12 is  0.24256393851305189\n",
      "TP with threshold 0.1 = 40\n",
      "FP with threshold 0.1 = 36\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.6837606837606838\n",
      "TP with threshold 0.2 = 34\n",
      "FP with threshold 0.2 = 42\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.6126126126126126\n",
      "TP with threshold 0.3 = 26\n",
      "FP with threshold 0.3 = 50\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.5048543689320388\n",
      "TP with threshold 0.4 = 10\n",
      "FP with threshold 0.4 = 66\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.22988505747126434\n",
      "TP with threshold 0.5 = 2\n",
      "FP with threshold 0.5 = 74\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.05063291139240506\n",
      "Mean F Measure = 0.41634912683380093\n",
      "f1 measure for image 13 is  0.41634912683380093\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.0\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.0\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.0\n",
      "f1 measure for image 14 is  0.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.0\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.0\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.0\n",
      "f1 measure for image 15 is  0.0\n",
      "TP with threshold 0.1 = 3\n",
      "FP with threshold 0.1 = 4\n",
      "FN with threshold 0.1 = 6\n",
      "F_measure for image with threshold=0.1 = 0.375\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 7\n",
      "FN with threshold 0.2 = 6\n",
      "F_measure for image with threshold=0.2 = 0.0\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 7\n",
      "FN with threshold 0.3 = 6\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 7\n",
      "FN with threshold 0.4 = 6\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 7\n",
      "FN with threshold 0.5 = 6\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.075\n",
      "f1 measure for image 16 is  0.075\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.0\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.0\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.0\n",
      "f1 measure for image 17 is  0.0\n",
      "TP with threshold 0.1 = 30\n",
      "FP with threshold 0.1 = 48\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.5504587155963303\n",
      "TP with threshold 0.2 = 26\n",
      "FP with threshold 0.2 = 52\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.4952380952380952\n",
      "TP with threshold 0.3 = 16\n",
      "FP with threshold 0.3 = 62\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.3368421052631579\n",
      "TP with threshold 0.4 = 9\n",
      "FP with threshold 0.4 = 69\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.20454545454545456\n",
      "TP with threshold 0.5 = 5\n",
      "FP with threshold 0.5 = 73\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.11904761904761904\n",
      "Mean F Measure = 0.3412263979381314\n",
      "f1 measure for image 18 is  0.3412263979381314\n",
      "TP with threshold 0.1 = 1\n",
      "FP with threshold 0.1 = 1\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.5\n",
      "TP with threshold 0.2 = 1\n",
      "FP with threshold 0.2 = 1\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.5\n",
      "TP with threshold 0.3 = 1\n",
      "FP with threshold 0.3 = 1\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.5\n",
      "TP with threshold 0.4 = 1\n",
      "FP with threshold 0.4 = 1\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.5\n",
      "TP with threshold 0.5 = 1\n",
      "FP with threshold 0.5 = 1\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.5\n",
      "Mean F Measure = 0.5\n",
      "f1 measure for image 19 is  0.5\n",
      "TP with threshold 0.1 = 7\n",
      "FP with threshold 0.1 = 9\n",
      "FN with threshold 0.1 = 4\n",
      "F_measure for image with threshold=0.1 = 0.5185185185185185\n",
      "TP with threshold 0.2 = 7\n",
      "FP with threshold 0.2 = 9\n",
      "FN with threshold 0.2 = 4\n",
      "F_measure for image with threshold=0.2 = 0.5185185185185185\n",
      "TP with threshold 0.3 = 6\n",
      "FP with threshold 0.3 = 10\n",
      "FN with threshold 0.3 = 4\n",
      "F_measure for image with threshold=0.3 = 0.4615384615384615\n",
      "TP with threshold 0.4 = 4\n",
      "FP with threshold 0.4 = 12\n",
      "FN with threshold 0.4 = 4\n",
      "F_measure for image with threshold=0.4 = 0.3333333333333333\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 16\n",
      "FN with threshold 0.5 = 4\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.36638176638176634\n",
      "f1 measure for image 20 is  0.36638176638176634\n",
      "TP with threshold 0.1 = 16\n",
      "FP with threshold 0.1 = 16\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.6530612244897959\n",
      "TP with threshold 0.2 = 12\n",
      "FP with threshold 0.2 = 20\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.5333333333333333\n",
      "TP with threshold 0.3 = 9\n",
      "FP with threshold 0.3 = 23\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.4285714285714286\n",
      "TP with threshold 0.4 = 2\n",
      "FP with threshold 0.4 = 30\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.11428571428571428\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 32\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.3458503401360544\n",
      "f1 measure for image 21 is  0.3458503401360544\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 22 is  1.0\n",
      "TP with threshold 0.1 = 38\n",
      "FP with threshold 0.1 = 25\n",
      "FN with threshold 0.1 = 4\n",
      "F_measure for image with threshold=0.1 = 0.7238095238095238\n",
      "TP with threshold 0.2 = 35\n",
      "FP with threshold 0.2 = 28\n",
      "FN with threshold 0.2 = 4\n",
      "F_measure for image with threshold=0.2 = 0.6862745098039216\n",
      "TP with threshold 0.3 = 26\n",
      "FP with threshold 0.3 = 37\n",
      "FN with threshold 0.3 = 4\n",
      "F_measure for image with threshold=0.3 = 0.5591397849462365\n",
      "TP with threshold 0.4 = 5\n",
      "FP with threshold 0.4 = 58\n",
      "FN with threshold 0.4 = 4\n",
      "F_measure for image with threshold=0.4 = 0.1388888888888889\n",
      "TP with threshold 0.5 = 1\n",
      "FP with threshold 0.5 = 62\n",
      "FN with threshold 0.5 = 4\n",
      "F_measure for image with threshold=0.5 = 0.029411764705882353\n",
      "Mean F Measure = 0.42750489443089057\n",
      "f1 measure for image 23 is  0.42750489443089057\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 24 is  1.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 25 is  1.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 26 is  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hedi Fendri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Hedi Fendri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 27 is  1.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 28 is  1.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 29 is  1.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 30 is  1.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.0\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.0\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.0\n",
      "f1 measure for image 31 is  0.0\n",
      "TP with threshold 0.1 = 28\n",
      "FP with threshold 0.1 = 26\n",
      "FN with threshold 0.1 = 4\n",
      "F_measure for image with threshold=0.1 = 0.6511627906976744\n",
      "TP with threshold 0.2 = 23\n",
      "FP with threshold 0.2 = 31\n",
      "FN with threshold 0.2 = 4\n",
      "F_measure for image with threshold=0.2 = 0.5679012345679013\n",
      "TP with threshold 0.3 = 7\n",
      "FP with threshold 0.3 = 47\n",
      "FN with threshold 0.3 = 4\n",
      "F_measure for image with threshold=0.3 = 0.21538461538461537\n",
      "TP with threshold 0.4 = 2\n",
      "FP with threshold 0.4 = 52\n",
      "FN with threshold 0.4 = 4\n",
      "F_measure for image with threshold=0.4 = 0.06666666666666667\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 54\n",
      "FN with threshold 0.5 = 4\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.3002230614633715\n",
      "f1 measure for image 32 is  0.3002230614633715\n",
      "TP with threshold 0.1 = 7\n",
      "FP with threshold 0.1 = 6\n",
      "FN with threshold 0.1 = 5\n",
      "F_measure for image with threshold=0.1 = 0.5599999999999999\n",
      "TP with threshold 0.2 = 5\n",
      "FP with threshold 0.2 = 8\n",
      "FN with threshold 0.2 = 5\n",
      "F_measure for image with threshold=0.2 = 0.4347826086956522\n",
      "TP with threshold 0.3 = 2\n",
      "FP with threshold 0.3 = 11\n",
      "FN with threshold 0.3 = 5\n",
      "F_measure for image with threshold=0.3 = 0.2\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 13\n",
      "FN with threshold 0.4 = 5\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 13\n",
      "FN with threshold 0.5 = 5\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.23895652173913043\n",
      "f1 measure for image 33 is  0.23895652173913043\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 34 is  1.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 35 is  1.0\n",
      "TP with threshold 0.1 = 2\n",
      "FP with threshold 0.1 = 2\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.5714285714285715\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 4\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.0\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 4\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 4\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 4\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.1142857142857143\n",
      "f1 measure for image 36 is  0.1142857142857143\n",
      "TP with threshold 0.1 = 5\n",
      "FP with threshold 0.1 = 5\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.625\n",
      "TP with threshold 0.2 = 4\n",
      "FP with threshold 0.2 = 6\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.5333333333333333\n",
      "TP with threshold 0.3 = 4\n",
      "FP with threshold 0.3 = 6\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.5333333333333333\n",
      "TP with threshold 0.4 = 1\n",
      "FP with threshold 0.4 = 9\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.16666666666666669\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 10\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.37166666666666665\n",
      "f1 measure for image 37 is  0.37166666666666665\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.0\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.0\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.0\n",
      "f1 measure for image 38 is  0.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 39 is  1.0\n",
      "TP with threshold 0.1 = 4\n",
      "FP with threshold 0.1 = 4\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.6153846153846154\n",
      "TP with threshold 0.2 = 4\n",
      "FP with threshold 0.2 = 4\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.6153846153846154\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 8\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 8\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 8\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.24615384615384617\n",
      "f1 measure for image 40 is  0.24615384615384617\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 41 is  1.0\n",
      "TP with threshold 0.1 = 10\n",
      "FP with threshold 0.1 = 23\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.45454545454545453\n",
      "TP with threshold 0.2 = 8\n",
      "FP with threshold 0.2 = 25\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.38095238095238093\n",
      "TP with threshold 0.3 = 3\n",
      "FP with threshold 0.3 = 30\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.16216216216216214\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 33\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 33\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.19953199953199954\n",
      "f1 measure for image 42 is  0.19953199953199954\n",
      "TP with threshold 0.1 = 5\n",
      "FP with threshold 0.1 = 5\n",
      "FN with threshold 0.1 = 2\n",
      "F_measure for image with threshold=0.1 = 0.588235294117647\n",
      "TP with threshold 0.2 = 3\n",
      "FP with threshold 0.2 = 7\n",
      "FN with threshold 0.2 = 2\n",
      "F_measure for image with threshold=0.2 = 0.4\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 10\n",
      "FN with threshold 0.3 = 2\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 10\n",
      "FN with threshold 0.4 = 2\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 10\n",
      "FN with threshold 0.5 = 2\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.1976470588235294\n",
      "f1 measure for image 43 is  0.1976470588235294\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.0\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.0\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.0\n",
      "f1 measure for image 44 is  0.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 45 is  1.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 46 is  1.0\n",
      "TP with threshold 0.1 = 0\n",
      "FP with threshold 0.1 = 0\n",
      "FN with threshold 0.1 = 0\n",
      "F_measure for image with threshold=0.1 = 1\n",
      "TP with threshold 0.2 = 0\n",
      "FP with threshold 0.2 = 0\n",
      "FN with threshold 0.2 = 0\n",
      "F_measure for image with threshold=0.2 = 1\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 0\n",
      "FN with threshold 0.3 = 0\n",
      "F_measure for image with threshold=0.3 = 1\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 0\n",
      "FN with threshold 0.4 = 0\n",
      "F_measure for image with threshold=0.4 = 1\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 0\n",
      "FN with threshold 0.5 = 0\n",
      "F_measure for image with threshold=0.5 = 1\n",
      "Mean F Measure = 1.0\n",
      "f1 measure for image 47 is  1.0\n",
      "TP with threshold 0.1 = 15\n",
      "FP with threshold 0.1 = 21\n",
      "FN with threshold 0.1 = 1\n",
      "F_measure for image with threshold=0.1 = 0.5769230769230769\n",
      "TP with threshold 0.2 = 5\n",
      "FP with threshold 0.2 = 31\n",
      "FN with threshold 0.2 = 1\n",
      "F_measure for image with threshold=0.2 = 0.2380952380952381\n",
      "TP with threshold 0.3 = 0\n",
      "FP with threshold 0.3 = 36\n",
      "FN with threshold 0.3 = 1\n",
      "F_measure for image with threshold=0.3 = 0.0\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 36\n",
      "FN with threshold 0.4 = 1\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 36\n",
      "FN with threshold 0.5 = 1\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.163003663003663\n",
      "f1 measure for image 48 is  0.163003663003663\n",
      "TP with threshold 0.1 = 2\n",
      "FP with threshold 0.1 = 2\n",
      "FN with threshold 0.1 = 2\n",
      "F_measure for image with threshold=0.1 = 0.5\n",
      "TP with threshold 0.2 = 2\n",
      "FP with threshold 0.2 = 2\n",
      "FN with threshold 0.2 = 2\n",
      "F_measure for image with threshold=0.2 = 0.5\n",
      "TP with threshold 0.3 = 1\n",
      "FP with threshold 0.3 = 3\n",
      "FN with threshold 0.3 = 2\n",
      "F_measure for image with threshold=0.3 = 0.28571428571428575\n",
      "TP with threshold 0.4 = 0\n",
      "FP with threshold 0.4 = 4\n",
      "FN with threshold 0.4 = 2\n",
      "F_measure for image with threshold=0.4 = 0.0\n",
      "TP with threshold 0.5 = 0\n",
      "FP with threshold 0.5 = 4\n",
      "FN with threshold 0.5 = 2\n",
      "F_measure for image with threshold=0.5 = 0.0\n",
      "Mean F Measure = 0.2571428571428572\n",
      "f1 measure for image 49 is  0.2571428571428572\n"
     ]
    }
   ],
   "source": [
    "(winW, winH) = (48, 48)\n",
    "prepared_data_path_unseen = './project-data/images/test/unseen/'\n",
    "dict_jason={}\n",
    "step=[18,24,30,36,42]\n",
    "prob=[0.6,0.7,0.8,0.95]\n",
    "for image_number in range(len(img_list_test)):\n",
    "    ground_truth_1=[]\n",
    "    for i in range(len(annotations_xmls_test[image_number])): \n",
    "        ground_truth_1.append(annotations_xmls_test[image_number][i]['bbox'])\n",
    "    img_name=img_list_test[image_number]\n",
    "    ID=image_number\n",
    "    im=cv2.imread(src_path_test+img_name)\n",
    "    all_boxes=[]\n",
    "    mean_F1=0\n",
    "    count_=0\n",
    "    old_mean_F1=0\n",
    "    size_step=-1\n",
    "    size_prob=-1\n",
    "    for stepSize in step : \n",
    "        size_step+=1\n",
    "        for prob_thresh in prob:\n",
    "            size_prob+=1\n",
    "            count=0\n",
    "            X_val_unseen_=[]\n",
    "            for (x, y, window) in sliding_window(im, stepSize, windowSize=(winW, winH)):\n",
    "                # if the window does not meet our desired window size, ignore it\n",
    "                if window.shape[0] != winH or window.shape[1] != winW:\n",
    "                    continue\n",
    "                name=img_name+\"_\"+str(count)\n",
    "                im=Image.fromarray(window)\n",
    "                X_val_unseen_.append(window/255)\n",
    "                b, g, r = im.split()\n",
    "                im= Image.merge(\"RGB\", (r, g, b))\n",
    "                #im.save(prepared_data_path_unseen+name+\"_0\"\".jpg\")   \n",
    "                count=count+1\n",
    "            X_val_unseen_=np.array(X_val_unseen_)\n",
    "            imgs_mean_val_unseen = np.mean(X_val_unseen_,axis=(0,1,2)).reshape((1,1,1,-1))\n",
    "            imgs_std_val_unseen = np.std(X_val_unseen_,axis=(0,1,2)).reshape((1,1,1,-1))\n",
    "            X_val_unseen_ = (X_val_unseen_ - imgs_mean_val_unseen) / imgs_std_val_unseen #.reshape(num_imgs, -1)\n",
    "            if(X_val_unseen_.shape[1]!=num_channels):\n",
    "                X_val_unseen_ = X_val_unseen_.transpose((0,3,1,2))\n",
    "            # Predict bounding boxes on the train images.\n",
    "            with torch.no_grad():\n",
    "                pred_y_unseen_logit = net.forward(torch.tensor(X_val_unseen_, dtype=torch.float32))\n",
    "                pred_y_unseen_logit = pred_y_unseen_logit.numpy()\n",
    "                pred_y_unseen_label = pred_y_unseen_logit>prob_thresh\n",
    "            im=cv2.imread(src_path_test+img_name)\n",
    "            count=0\n",
    "            pred_box=[]\n",
    "            x_old=0\n",
    "            y_old=0\n",
    "            old_iou=0\n",
    "            arg_iou_old=0\n",
    "            for (x, y, window) in sliding_window(im, stepSize, windowSize=(winW, winH)):\n",
    "                # if the window does not meet our desired window size, ignore it\n",
    "                if window.shape[0] != winH or window.shape[1] != winW:\n",
    "                    continue\n",
    "                if pred_y_unseen_label[count]==1:\n",
    "                    if (len(ground_truth_1)>0):\n",
    "                        v=compute_iou_box(ground_truth_1,[x,y,48,48])\n",
    "                        iou_new=np.max(v)\n",
    "                        arg_iou=np.argmax(v)\n",
    "                        if  (arg_iou !=arg_iou_old):\n",
    "                            pred_box.append([x,y,48,48])\n",
    "                            old_iou=iou_new \n",
    "                            arg_iou_old=arg_iou\n",
    "                count=count+1\n",
    "            #print(\"The number of predicted varroas is {} and the number of ground truth is {}\".format(len(pred_box),len(ground_truth_1)))\n",
    "            mean_F1=(compute_f1(pred_box,ground_truth_1,0))\n",
    "            if mean_F1>old_mean_F1:\n",
    "                all_boxes=pred_box\n",
    "                old_mean_F1=mean_F1\n",
    "    #plot_predictedVsGround(im,ground_truth_1,all_boxes)\n",
    "    f1_final=compute_f1(all_boxes,ground_truth_1,1)\n",
    "    dict_jason[ID]=all_boxes\n",
    "    print(\"f1 measure for image {} is \".format(image_number),f1_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peformance on the testing data on Eval \n",
    "Total mean F-measure (Same metric used in the Eval plateform) = 0.53\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def generate_pred_json(data, tag='baseline'):\n",
    "    '''\n",
    "    Input\n",
    "    - data: Is a dictionary d, such that:\n",
    "          d = { \n",
    "              \"ID_1\": [], \n",
    "              \"ID_2\": [[x_21, y_21, w_21, h_21], [x_22, y_22, w_22, h_22]], \n",
    "              ... \n",
    "              \"ID_i\": [[x_i1, y_i1, w_i1, h_i1], ..., [x_iJ, y_iJ, w_iJ, h_iJ]],\n",
    "              ... \n",
    "              \"ID_N\": [[x_N1, y_N1, w_N1, h_N1]],\n",
    "          }\n",
    "          where ID is the string id of the image (e.i. 5a05e86fa07d56baef59b1cb_32.00px_1) and the value the Kx4 \n",
    "          array of intergers for the K predicted bounding boxes (e.g. [[170, 120, 15, 15]])\n",
    "    - tag: (optional) string that will be added to the name of the json file.\n",
    "    Output\n",
    "      Create a json file, \"prediction_[tag].json\", conatining the prediction to EvalAI format.\n",
    "    '''\n",
    "    unvalid_key = []\n",
    "    _data = data.copy()\n",
    "    for key, value in _data.items():\n",
    "        try:\n",
    "            # Try to convert to numpy array and cast as closest int\n",
    "            #print(key)\n",
    "            v = np.around(np.array(value)).astype(int)\n",
    "            # Check is it is a 2d array with 4 columns (x,y,w,h)\n",
    "            if v.ndim != 2 or v.shape[1] != 4:\n",
    "                unvalid_key.append(key)\n",
    "            # Id must be a string\n",
    "            if not isinstance(key, str):\n",
    "                unvalid_key.append(key)\n",
    "            _data[key] = v.tolist()\n",
    "        # Deal with not consistant array size and empty predictions\n",
    "        except (ValueError, TypeError):\n",
    "            unvalid_key.append(key)\n",
    "    # Remove unvalid key from dictionnary\n",
    "    for key in unvalid_key: del _data[key]\n",
    "    \n",
    "    with open('prediction_{}.json'.format(tag), 'w') as outfile:\n",
    "        json.dump(_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_4_jason={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_list_test)):\n",
    "    imname=img_list_test[i]\n",
    "    dict_4_jason[imname[:-4]]=dict_final[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_pred_json(dict_4_jason,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result can be verified by checking the EVal plateform on testing phase : \n",
    "The Final Result on testing considering the same metric EVal is : 0.53 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
